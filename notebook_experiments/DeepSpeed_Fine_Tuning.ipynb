{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate_profile.pl  mpif90\t    orte-clean\t oshc++\t\t shmemCC\r\n",
      "mpiCC\t\t      mpifort\t    orte-info\t oshcc\t\t shmemc++\r\n",
      "mpic++\t\t      mpirun\t    orte-server  oshcxx\t\t shmemcc\r\n",
      "mpicc\t\t      ompi-clean    ortecc\t oshfort\t shmemcxx\r\n",
      "mpicxx\t\t      ompi-server   orted\t oshmem_info\t shmemfort\r\n",
      "mpiexec\t\t      ompi_info     orterun\t oshrun\t\t shmemrun\r\n",
      "mpif77\t\t      opal_wrapper  oshCC\t profile2mat.pl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /share/apps/openmpi/4.1.1/intel/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load cuda/11.6.2\n",
    "!module load cudnn/8.6.0.163-cuda11\n",
    "!module load openmpi/intel/4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Hostname: gr052.hpc.nyu.edu\n",
      "Processor: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz\n",
      "RAM: 146Gi\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --use-feature=2020-resolver pynvml zstandard datasets psutil transformers torch bitsandbytes accelerate loralib deepspeed peft\n",
    "!echo \"Hostname: $(hostname)\"\n",
    "!echo \"Processor: $(lscpu | grep 'Model name' | awk -F ':' '{print $2}' | xargs)\"\n",
    "!echo \"RAM: $(free -h | grep 'Mem:' | awk '{print $4}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mpi4py\n",
      "  Using cached mpi4py-3.1.4.tar.gz (2.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
      "  Building wheel for mpi4py (PEP 517) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /share/apps/python/3.8.6/intel/bin/python /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /state/partition1/job-32915051/tmpip3s6pdi\n",
      "       cwd: /state/partition1/job-32915051/pip-install-vmo4xya4/mpi4py\n",
      "  Complete output (148 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_src\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-cpython-38\n",
      "  creating build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/__init__.py -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/__main__.py -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/bench.py -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/run.py -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  creating build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__init__.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__main__.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_base.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_core.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_lib.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/aplus.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/pool.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/server.py -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  creating build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  copying src/mpi4py/util/__init__.py -> build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  copying src/mpi4py/util/dtlib.py -> build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  copying src/mpi4py/util/pkl5.py -> build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  copying src/mpi4py/py.typed -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/MPI.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/__init__.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/__main__.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/bench.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/dl.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/run.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/MPI.pxd -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/__init__.pxd -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  copying src/mpi4py/libmpi.pxd -> build/lib.linux-x86_64-cpython-38/mpi4py\n",
      "  creating build/lib.linux-x86_64-cpython-38/mpi4py/include\n",
      "  creating build/lib.linux-x86_64-cpython-38/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.MPI.h -> build/lib.linux-x86_64-cpython-38/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.MPI_api.h -> build/lib.linux-x86_64-cpython-38/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.h -> build/lib.linux-x86_64-cpython-38/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi4py.i -> build/lib.linux-x86_64-cpython-38/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/include/mpi4py/mpi.pxi -> build/lib.linux-x86_64-cpython-38/mpi4py/include/mpi4py\n",
      "  copying src/mpi4py/futures/__init__.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/__main__.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_core.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/_lib.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/aplus.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/pool.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/futures/server.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/futures\n",
      "  copying src/mpi4py/util/__init__.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  copying src/mpi4py/util/dtlib.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  copying src/mpi4py/util/pkl5.pyi -> build/lib.linux-x86_64-cpython-38/mpi4py/util\n",
      "  running build_clib\n",
      "  MPI configuration: [mpi] from 'mpi.cfg'\n",
      "  checking for library 'lmpe' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -llmpe -o _configtest\n",
      "  /usr/bin/ld: cannot find -llmpe\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'mpe' dylib library\n",
      "  creating build/temp.linux-x86_64-cpython-38\n",
      "  creating build/temp.linux-x86_64-cpython-38/src\n",
      "  creating build/temp.linux-x86_64-cpython-38/src/lib-pmpi\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c src/lib-pmpi/mpe.c -o build/temp.linux-x86_64-cpython-38/src/lib-pmpi/mpe.o\n",
      "  creating build/lib.linux-x86_64-cpython-38/mpi4py/lib-pmpi\n",
      "  gcc -shared -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -Wl,--no-as-needed build/temp.linux-x86_64-cpython-38/src/lib-pmpi/mpe.o -o build/lib.linux-x86_64-cpython-38/mpi4py/lib-pmpi/libmpe.so\n",
      "  checking for library 'vt-mpi' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -lvt-mpi -o _configtest\n",
      "  /usr/bin/ld: cannot find -lvt-mpi\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for library 'vt.mpi' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -lvt.mpi -o _configtest\n",
      "  /usr/bin/ld: cannot find -lvt.mpi\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'vt' dylib library\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c src/lib-pmpi/vt.c -o build/temp.linux-x86_64-cpython-38/src/lib-pmpi/vt.o\n",
      "  gcc -shared -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -Wl,--no-as-needed build/temp.linux-x86_64-cpython-38/src/lib-pmpi/vt.o -o build/lib.linux-x86_64-cpython-38/mpi4py/lib-pmpi/libvt.so\n",
      "  checking for library 'vt-mpi' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -lvt-mpi -o _configtest\n",
      "  /usr/bin/ld: cannot find -lvt-mpi\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for library 'vt.mpi' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -lvt.mpi -o _configtest\n",
      "  /usr/bin/ld: cannot find -lvt.mpi\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'vt-mpi' dylib library\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c src/lib-pmpi/vt-mpi.c -o build/temp.linux-x86_64-cpython-38/src/lib-pmpi/vt-mpi.o\n",
      "  gcc -shared -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -Wl,--no-as-needed build/temp.linux-x86_64-cpython-38/src/lib-pmpi/vt-mpi.o -o build/lib.linux-x86_64-cpython-38/mpi4py/lib-pmpi/libvt-mpi.so\n",
      "  checking for library 'vt-hyb' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -lvt-hyb -o _configtest\n",
      "  /usr/bin/ld: cannot find -lvt-hyb\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  checking for library 'vt.ompi' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -lvt.ompi -o _configtest\n",
      "  /usr/bin/ld: cannot find -lvt.ompi\n",
      "  collect2: error: ld returned 1 exit status\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  building 'vt-hyb' dylib library\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -c src/lib-pmpi/vt-hyb.c -o build/temp.linux-x86_64-cpython-38/src/lib-pmpi/vt-hyb.o\n",
      "  gcc -shared -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -Wl,--no-as-needed build/temp.linux-x86_64-cpython-38/src/lib-pmpi/vt-hyb.o -o build/lib.linux-x86_64-cpython-38/mpi4py/lib-pmpi/libvt-hyb.so\n",
      "  running build_ext\n",
      "  MPI configuration: [mpi] from 'mpi.cfg'\n",
      "  checking for dlopen() availability ...\n",
      "  checking for header 'dlfcn.h' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/share/apps/python/3.8.6/intel/include/python3.8 -c _configtest.c -o _configtest.o\n",
      "  success!\n",
      "  removing: _configtest.c _configtest.o\n",
      "  success!\n",
      "  checking for library 'dl' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/share/apps/python/3.8.6/intel/include/python3.8 -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -L/share/apps/python/3.8.6/intel/lib -Lbuild/temp.linux-x86_64-cpython-38 -Wl,--enable-new-dtags,-R/share/apps/python/3.8.6/intel/lib -ldl -o _configtest\n",
      "  success!\n",
      "  removing: _configtest.c _configtest.o _configtest\n",
      "  checking for function 'dlopen' ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/share/apps/python/3.8.6/intel/include/python3.8 -c _configtest.c -o _configtest.o\n",
      "  gcc _configtest.o -L/share/apps/python/3.8.6/intel/lib -Lbuild/temp.linux-x86_64-cpython-38 -Wl,--enable-new-dtags,-R/share/apps/python/3.8.6/intel/lib -ldl -o _configtest\n",
      "  success!\n",
      "  removing: _configtest.c _configtest.o _configtest\n",
      "  building 'mpi4py.dl' extension\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DHAVE_DLFCN_H=1 -DHAVE_DLOPEN=1 -I/share/apps/python/3.8.6/intel/include/python3.8 -c src/dynload.c -o build/temp.linux-x86_64-cpython-38/src/dynload.o\n",
      "  gcc -shared -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib -L/share/apps/intel/19.1.2/mkl/lib/intel64 -L/share/apps/intel/19.1.2/lib/intel64 -L/share/apps/python/3.8.6/intel/lib build/temp.linux-x86_64-cpython-38/src/dynload.o -L/share/apps/python/3.8.6/intel/lib -Lbuild/temp.linux-x86_64-cpython-38 -Wl,--enable-new-dtags,-R/share/apps/python/3.8.6/intel/lib -ldl -o build/lib.linux-x86_64-cpython-38/mpi4py/dl.cpython-38-x86_64-linux-gnu.so\n",
      "  checking for MPI compile and link ...\n",
      "  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/share/apps/python/3.8.6/intel/include/python3.8 -c _configtest.c -o _configtest.o\n",
      "  _configtest.c:2:10: fatal error: mpi.h: No such file or directory\n",
      "   #include <mpi.h>\n",
      "            ^~~~~~~\n",
      "  compilation terminated.\n",
      "  failure.\n",
      "  removing: _configtest.c _configtest.o\n",
      "  error: Cannot compile MPI programs. Check your configuration!!!\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for mpi4py\u001b[0m\n",
      "\u001b[?25hFailed to build mpi4py\n",
      "\u001b[31mERROR: Could not build wheels for mpi4py which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.1.2 is available.\r\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!export CC=/share/apps/openmpi/4.1.1/intel/bin/mpicc\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/share/apps/openmpi/4.1.1/intel/bin/mpicc:/share/apps/openmpi/4.1.1/intel/lib:/share/apps/mvapich2/2.3.6/intel/lib\n",
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Quadro RTX 8000\n",
      "GPU Memory: 0MiB/46080MiB\n"
     ]
    }
   ],
   "source": [
    "!echo \"GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)\"\n",
    "!echo \"GPU Memory: $(nvidia-smi | grep MiB |  awk '{print $9 $10 $11}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 01:35:04,325 - INFO - <module>:129 - Is Flash Attention Enabled: True\n",
      "2023-04-30 01:35:04,326 - INFO - <module>:130 - Is Mem Efficient SDP Enabled: False\n",
      "2023-04-30 01:35:04,326 - INFO - <module>:131 - Is Math SDP Enabled: False\n",
      "2023-04-30 01:35:04,329 - INFO - print_gpu_utilization:154 - GPU memory occupied: 4648 MB.\n",
      "2023-04-30 01:35:04,492 - INFO - run_data_pipeline:252 - Baseline: RAM used: 2291.26 MB\n",
      "2023-04-30 01:35:04,753 - WARNING - download_and_prepare:817 - Found cached dataset json (/scratch/vgn2004/fine_tuning/datasets/json/default-3ed31d147447149f/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "2023-04-30 01:35:04,767 - INFO - run_data_pipeline:274 - RAM used: 2296.86 MB\n",
      "2023-04-30 01:35:04,768 - INFO - run_data_pipeline:275 - Dataset sample: {'meta': {'APPLICATION_ID': 100075}, 'text': \"ACF's Office of Refugee Resettlement (ORR) administers a variety of social service programs intended to connect newly resettled refugees with critical resources, help them become economically self-sufficient, and help them integrate into American society. One such program is the Refugee Cash Assistance (RCA) program, which provides both financial support and social services to newly resettled refugees. Refugee Cash Assistance is similar to TANF in that both are cash assistance programs that provide services aimed at promoting self-sufficiency; however the content, mode of delivery and rules surrounding these services vary significantly by state and locality. Some counties and states have reportedly integrated the delivery of TANF and RCA in a purposeful way to better serve refugees. However, there is little documented information on the extent to which refugees access benefits and services through TANF and RCA, differences in refugee characteristics between the two programs, how outcomes compare for refugees served under these two programs, whether integration of these programs holds promise for refugee self-sufficiency, and whether data is available to answer these questions. The Understanding the Intersection Between TANF and Refugee Cash Assistance Services project aims to improve understanding of how RCA and TANF serve refugee populations, how these programs intersect, and how these programs may be related to refugee self-sufficiency and employment outcomes. In fall 2014 ACF launched this descriptive study to document the similarities and differences between cash assistance and associated social services offered under RCA and TANF across different selected jurisdictions. The study aims to better understand the population of refugees served by TANF and RCA, and the major differences in programmatic services associated with these two programs. The study will also explore how states and localities have coordinated TANF and RCA programs to deliver social services to refugees and whether these approaches hold promise for long-term job stability and economic self-sufficiency among refugees. This field study will provide a deeper understanding of current social service delivery systems serving refugees and will help to identify gaps in existing knowledge and data around these systems. By improving knowledge of these programs and participant experiences, ACF hopes to move toward better serving this population. The project is being conducted by Abt Associates and MEF Associates.\"}\n",
      "2023-04-30 01:35:04,768 - INFO - run_data_pipeline:277 - Dataset size (cache file) : 2.01 GB\n",
      "2023-04-30 01:35:04,782 - INFO - create_dataloaders:222 - Creating data loaders...\n",
      "2023-04-30 01:35:04,783 - INFO - create_dataloaders:235 - Time taken to create data loaders: 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-30 01:35:05,335] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 0.49B parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 01:35:05,516 - INFO - create_or_load_model:311 - Model: gpt2\n",
      "2023-04-30 01:35:05,517 - INFO - create_or_load_model:312 - Total Parameters: 0.00 million\n",
      "2023-04-30 01:35:05,517 - INFO - create_or_load_model:313 - Trainable Parameters: 0.00 million\n",
      "2023-04-30 01:35:05,519 - INFO - create_or_load_model:314 - Memory Footprint: 12.58296 MB\n",
      "2023-04-30 01:35:05,519 - INFO - create_or_load_model:315 - Model is on device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-30 01:35:05,520] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown\n",
      "[2023-04-30 01:35:05,529] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Time to load fused_adam op: 0.0013434886932373047 seconds\n",
      "[2023-04-30 01:35:05,655] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2023-04-30 01:35:05,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2023-04-30 01:35:05,664] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2023-04-30 01:35:05,665] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/vgn2004/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-30 01:35:05,801] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-04-30 01:35:05,803] [INFO] [utils.py:786:see_memory_usage] MA 3.49 GB         Max_MA 3.65 GB         CA 3.74 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:05,803] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:05,805] [INFO] [stage3.py:113:__init__] Reduce bucket size 500,000,000\n",
      "[2023-04-30 01:35:05,805] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50,000,000\n",
      "Time to load utils op: 0.0008342266082763672 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/vgn2004/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-30 01:35:05,897] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-04-30 01:35:05,898] [INFO] [utils.py:786:see_memory_usage] MA 3.49 GB         Max_MA 3.49 GB         CA 3.74 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:05,899] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "Parameter Offload: Total persistent parameters: 121344 in 98 params\n",
      "[2023-04-30 01:35:05,996] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-04-30 01:35:05,998] [INFO] [utils.py:786:see_memory_usage] MA 3.49 GB         Max_MA 3.49 GB         CA 3.74 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:05,998] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,087] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-04-30 01:35:06,089] [INFO] [utils.py:786:see_memory_usage] MA 3.49 GB         Max_MA 3.49 GB         CA 3.74 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:06,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,509] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2023-04-30 01:35:06,510] [INFO] [utils.py:786:see_memory_usage] MA 3.49 GB         Max_MA 3.49 GB         CA 3.74 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:06,511] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,598] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-04-30 01:35:06,600] [INFO] [utils.py:786:see_memory_usage] MA 3.49 GB         Max_MA 3.49 GB         CA 3.74 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:06,600] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,689] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\n",
      "[2023-04-30 01:35:06,690] [INFO] [utils.py:786:see_memory_usage] MA 3.96 GB         Max_MA 4.19 GB         CA 4.44 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:06,691] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.33 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,787] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-04-30 01:35:06,789] [INFO] [utils.py:786:see_memory_usage] MA 3.96 GB         Max_MA 3.96 GB         CA 4.44 GB         Max_CA 4 GB \n",
      "[2023-04-30 01:35:06,789] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.3 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,880] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-04-30 01:35:06,881] [INFO] [utils.py:786:see_memory_usage] MA 4.88 GB         Max_MA 5.35 GB         CA 5.84 GB         Max_CA 6 GB \n",
      "[2023-04-30 01:35:06,882] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.3 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:06,882] [INFO] [stage3.py:366:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2023-04-30 01:35:07,015] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-04-30 01:35:07,016] [INFO] [utils.py:786:see_memory_usage] MA 6.05 GB         Max_MA 6.19 GB         CA 6.77 GB         Max_CA 7 GB \n",
      "[2023-04-30 01:35:07,016] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 53.3 GB, percent = 14.1%\n",
      "[2023-04-30 01:35:07,017] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2023-04-30 01:35:07,017] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2023-04-30 01:35:07,018] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x14aa6a341fa0>\n",
      "[2023-04-30 01:35:07,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2023-04-30 01:35:07,019] [INFO] [config.py:953:print] DeepSpeedEngine configuration:\n",
      "[2023-04-30 01:35:07,019] [INFO] [config.py:957:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": true, \n",
      "    \"contiguous_memory_optimization\": true, \n",
      "    \"cpu_checkpointing\": true, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-04-30 01:35:07,020] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-04-30 01:35:07,020] [INFO] [config.py:957:print]   amp_enabled .................. False\n",
      "[2023-04-30 01:35:07,020] [INFO] [config.py:957:print]   amp_params ................... False\n",
      "[2023-04-30 01:35:07,021] [INFO] [config.py:957:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-04-30 01:35:07,021] [INFO] [config.py:957:print]   bfloat16_enabled ............. False\n",
      "[2023-04-30 01:35:07,021] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-04-30 01:35:07,021] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-04-30 01:35:07,022] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-04-30 01:35:07,022] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14aa6a342d30>\n",
      "[2023-04-30 01:35:07,022] [INFO] [config.py:957:print]   communication_data_type ...... None\n",
      "[2023-04-30 01:35:07,023] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-04-30 01:35:07,023] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False\n",
      "[2023-04-30 01:35:07,023] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False\n",
      "[2023-04-30 01:35:07,024] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-04-30 01:35:07,024] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False\n",
      "[2023-04-30 01:35:07,024] [INFO] [config.py:957:print]   dataloader_drop_last ......... False\n",
      "[2023-04-30 01:35:07,024] [INFO] [config.py:957:print]   disable_allgather ............ False\n",
      "[2023-04-30 01:35:07,025] [INFO] [config.py:957:print]   dump_state ................... False\n",
      "[2023-04-30 01:35:07,025] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-30 01:35:07,025] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False\n",
      "[2023-04-30 01:35:07,026] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-04-30 01:35:07,026] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-04-30 01:35:07,026] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-04-30 01:35:07,027] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-04-30 01:35:07,027] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-04-30 01:35:07,027] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-04-30 01:35:07,027] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False\n",
      "[2023-04-30 01:35:07,028] [INFO] [config.py:957:print]   elasticity_enabled ........... False\n",
      "[2023-04-30 01:35:07,028] [INFO] [config.py:957:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-04-30 01:35:07,028] [INFO] [config.py:957:print]   fp16_auto_cast ............... True\n",
      "[2023-04-30 01:35:07,029] [INFO] [config.py:957:print]   fp16_enabled ................. True\n",
      "[2023-04-30 01:35:07,029] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-04-30 01:35:07,029] [INFO] [config.py:957:print]   global_rank .................. 0\n",
      "[2023-04-30 01:35:07,030] [INFO] [config.py:957:print]   grad_accum_dtype ............. None\n",
      "[2023-04-30 01:35:07,030] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 8\n",
      "[2023-04-30 01:35:07,030] [INFO] [config.py:957:print]   gradient_clipping ............ 0.0\n",
      "[2023-04-30 01:35:07,030] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-04-30 01:35:07,031] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-04-30 01:35:07,031] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-04-30 01:35:07,031] [INFO] [config.py:957:print]   load_universal_checkpoint .... False\n",
      "[2023-04-30 01:35:07,032] [INFO] [config.py:957:print]   loss_scale ................... 0\n",
      "[2023-04-30 01:35:07,032] [INFO] [config.py:957:print]   memory_breakdown ............. False\n",
      "[2023-04-30 01:35:07,032] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-04-30 01:35:07,033] [INFO] [config.py:957:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-04-30 01:35:07,033] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-04-30 01:35:07,033] [INFO] [config.py:957:print]   optimizer_name ............... adamw\n",
      "[2023-04-30 01:35:07,033] [INFO] [config.py:957:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08}\n",
      "[2023-04-30 01:35:07,034] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-04-30 01:35:07,034] [INFO] [config.py:957:print]   pld_enabled .................. False\n",
      "[2023-04-30 01:35:07,034] [INFO] [config.py:957:print]   pld_params ................... False\n",
      "[2023-04-30 01:35:07,035] [INFO] [config.py:957:print]   prescale_gradients ........... False\n",
      "[2023-04-30 01:35:07,035] [INFO] [config.py:957:print]   scheduler_name ............... WarmupLR\n",
      "[2023-04-30 01:35:07,035] [INFO] [config.py:957:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 3e-05, 'warmup_num_steps': 50}\n",
      "[2023-04-30 01:35:07,036] [INFO] [config.py:957:print]   sparse_attention ............. None\n",
      "[2023-04-30 01:35:07,036] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False\n",
      "[2023-04-30 01:35:07,036] [INFO] [config.py:957:print]   steps_per_print .............. 100\n",
      "[2023-04-30 01:35:07,036] [INFO] [config.py:957:print]   train_batch_size ............. 128\n",
      "[2023-04-30 01:35:07,037] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  16\n",
      "[2023-04-30 01:35:07,037] [INFO] [config.py:957:print]   use_node_local_storage ....... False\n",
      "[2023-04-30 01:35:07,037] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False\n",
      "[2023-04-30 01:35:07,038] [INFO] [config.py:957:print]   world_size ................... 1\n",
      "[2023-04-30 01:35:07,038] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False\n",
      "[2023-04-30 01:35:07,038] [INFO] [config.py:957:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True\n",
      "[2023-04-30 01:35:07,039] [INFO] [config.py:957:print]   zero_enabled ................. True\n",
      "[2023-04-30 01:35:07,039] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-04-30 01:35:07,039] [INFO] [config.py:957:print]   zero_optimization_stage ...... 3\n",
      "[2023-04-30 01:35:07,040] [INFO] [config.py:943:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08\n",
      "        }\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true, \n",
      "        \"cpu_checkpointing\": true, \n",
      "        \"contiguous_memory_optimization\": true\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 3e-05, \n",
      "            \"warmup_num_steps\": 50\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"steps_per_print\": 100, \n",
      "    \"train_micro_batch_size_per_gpu\": 16\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/vgn2004/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load utils op: 0.006386995315551758 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]2023-04-30 01:35:07,048 - INFO - train:399 - Epoch: 1/25\n",
      "\n",
      "  0%|          | 0/3466 [00:00<?, ?it/s]\u001b[A2023-04-30 01:35:16,257 - INFO - print_gpu_utilization:154 - GPU memory occupied: 8134 MB.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  0%|          | 0/3466 [00:01<?, ?it/s]\n",
      "  0%|          | 0/25 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithCrossAttentions' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0dce2c280d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;31m#     logging.info(f\"Initial Text:\\n{generated_text}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0dce2c280d23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, data_dict)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;31m#             with torch.cuda.amp.autocast(dtype=torch.float16):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deepspeed/utils/nvtx.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mget_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_push\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mget_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_pop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, allreduce_gradients, release_loss, retain_graph, scale_wrt_gas)\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;31m# scale loss w.r.t. gradient accumulation if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscale_wrt_gas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale_loss_by_gas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \u001b[0;31m# Log training Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithCrossAttentions' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "# Change net ID here to use your scratch folder\n",
    "ENV = \"dev\"\n",
    "NET_ID = \"vgn2004\"\n",
    "ROOT_PATH = f\"/scratch/{NET_ID}/fine_tuning\"\n",
    "\n",
    "# Global configurations\n",
    "config = {\n",
    "    \"DATASET_URL\": \"https://the-eye.eu/public/AI/pile_v2/data\",\n",
    "    \"DATASET_NAME\": \"NIH_ExPORTER_awarded_grant_text\",\n",
    "    \"NUM_WORKERS\": 8,\n",
    "    \"DATASET_SPLIT_RATIO\": 0.9,\n",
    "    \"PADDING_STRATEGY\": \"max_length\",\n",
    "    \"MAX_TOKENS\": 128,\n",
    "    \"MODEL_NAME\": \"gpt2\",\n",
    "    \"TOKENIZED_NAME\": \"autos\",\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"NUM_EPOCHS\": 25,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"EPSILON\": 1e-8,\n",
    "    \"SAMPLING_INTERVAL\": 64,\n",
    "    \"CHECKPOINTING_INTERVAL\": 100,\n",
    "    \"VALIDATION_INTERVAL\": 500,\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 8,\n",
    "}\n",
    "\n",
    "deepspeed_config = {\n",
    "    \"fp16\": {\"enabled\": True, \"auto_cast\": True},\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\"lr\": 3e-5, \"betas\": [0.9, 0.999], \"eps\": 1e-8},\n",
    "    },\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": True,\n",
    "        \"cpu_checkpointing\": True,\n",
    "        \"contiguous_memory_optimization\": True,\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\"warmup_min_lr\": 0, \"warmup_max_lr\": 3e-5, \"warmup_num_steps\": 50},\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"steps_per_print\": 100,\n",
    "    \"train_micro_batch_size_per_gpu\": 16,\n",
    "}\n",
    "\n",
    "# ds_config = {\n",
    "#     \"fp16\": {\"enabled\": True},\n",
    "#     \"zero_optimization\": {\n",
    "#         \"stage\": 3,\n",
    "#         \"allgather_partitions\": True,\n",
    "#         \"allgather_bucket_size\": 5e8,\n",
    "#         \"overlap_comm\": True,\n",
    "#         \"reduce_scatter\": True,\n",
    "#         \"reduce_bucket_size\": 5e8,\n",
    "#         \"contiguous_gradients\": True\n",
    "#     },\n",
    "#     \"optimizer\": {\n",
    "#         \"type\": \"AdamW\",\n",
    "#         \"params\": {\n",
    "#             \"lr\": 3e-5,\n",
    "#             \"betas\": [0.9, 0.999],\n",
    "#             \"eps\": 1e-8\n",
    "#         }\n",
    "#     },\n",
    "#     \"scheduler\": {\n",
    "#         \"type\": \"WarmupLR\",\n",
    "#         \"params\": {\n",
    "#             \"warmup_min_lr\": 0,\n",
    "#             \"warmup_max_lr\": 3e-5,\n",
    "#             \"warmup_num_steps\": 1000\n",
    "#         }\n",
    "#     },\n",
    "#     \"steps_per_print\": 100,\n",
    "#     \"gradient_accumulation_steps\": 1,\n",
    "#     \"train_batch_size\": 16,\n",
    "#     \"train_micro_batch_size_per_gpu\": 4,\n",
    "#     \"wall_clock_breakdown\": False\n",
    "# }\n",
    "\n",
    "# Ensure that packages can be found\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, f\"/home/{NET_ID}/.local/lib/python3.8/site-packages\")\n",
    "\n",
    "# Ensure that GPU can be found\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
    "\n",
    "# Setup logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s\"\n",
    ")\n",
    "\n",
    "# Packages for profiling\n",
    "import random\n",
    "import psutil\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "from pynvml import *\n",
    "\n",
    "# Packages for data loading\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Core packages\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "logging.info(f\"Is Flash Attention Enabled: {torch.backends.cuda.flash_sdp_enabled()}\")\n",
    "logging.info(\n",
    "    f\"Is Mem Efficient SDP Enabled: {torch.backends.cuda.mem_efficient_sdp_enabled()}\"\n",
    ")\n",
    "logging.info(f\"Is Math SDP Enabled: {torch.backends.cuda.math_sdp_enabled()}\")\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "# dist.init_process_group(backend=None)\n",
    "\n",
    "import deepspeed\n",
    "import bitsandbytes.optim as bnb_optim\n",
    "\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.optimization import Adafactor\n",
    "from transformers.deepspeed import HfDeepSpeedConfig\n",
    "\n",
    "\n",
    "# Unused imports (TODO)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "# Get GPU Utilization\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    logging.info(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "# Returns RAM usage in MB\n",
    "def get_ram_usage():\n",
    "    return psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "\n",
    "# Takes a batch of inputs and runs the tokenizer on them\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=config[\"PADDING_STRATEGY\"],\n",
    "        truncation=True,\n",
    "        max_length=config[\"MAX_TOKENS\"],\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Tokenizes dataset and creates train and validation split\n",
    "def preprocess_data(dataset, tokenizer):\n",
    "    tokenized_dataset_path = f\"{ROOT_PATH}/datasets/tokenized_{config['DATASET_NAME']}_{config['TOKENIZED_NAME']}\"\n",
    "    train_dataset_path = f\"{tokenized_dataset_path}_train\"\n",
    "    valid_dataset_path = f\"{tokenized_dataset_path}_valid\"\n",
    "    if os.path.exists(train_dataset_path) and os.path.exists(valid_dataset_path):\n",
    "        train_dataset = load_from_disk(train_dataset_path)\n",
    "        valid_dataset = load_from_disk(valid_dataset_path)\n",
    "        return train_dataset, valid_dataset\n",
    "\n",
    "    logger.info(f\"Tokenizing the dataset...\")\n",
    "    start_time = time()\n",
    "    try:\n",
    "        tokenized_dataset = load_from_disk(tokenized_dataset_path)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            fn_kwargs={\"tokenizer\": tokenizer},\n",
    "            batched=True,\n",
    "            num_proc=8,\n",
    "            remove_columns=[\"text\", \"meta\"],\n",
    "        )\n",
    "        tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "\n",
    "    elapsed_time = time() - start_time\n",
    "    logger.info(f\"Time taken to tokenize the dataset: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    logger.info(f\"Splitting the dataset...\")\n",
    "    start_time = time()\n",
    "\n",
    "    if os.path.exists(train_dataset_path) and os.path.exists(valid_dataset_path):\n",
    "        train_dataset = load_from_disk(train_dataset_path)\n",
    "        valid_dataset = load_from_disk(valid_dataset_path)\n",
    "    else:\n",
    "        train_size = int(config[\"DATASET_SPLIT_RATIO\"] * len(tokenized_dataset))\n",
    "        datasets = DatasetDict(\n",
    "            {\n",
    "                \"train\": Dataset.from_dict(tokenized_dataset[:train_size]),\n",
    "                \"valid\": Dataset.from_dict(tokenized_dataset[train_size:]),\n",
    "            }\n",
    "        )\n",
    "        train_dataset = datasets[\"train\"]\n",
    "        valid_dataset = datasets[\"valid\"]\n",
    "        train_dataset.save_to_disk(train_dataset_path)\n",
    "        valid_dataset.save_to_disk(valid_dataset_path)\n",
    "    elapsed_time = time() - start_time\n",
    "    logger.info(\n",
    "        f\"Time taken to split the datasets (or load pre-split datasets): {elapsed_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "# Creates data loaders\n",
    "def create_dataloaders(train_dataset, valid_dataset, data_collator):\n",
    "    logger.info(f\"Creating data loaders...\")\n",
    "    start_time = time()\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=config[\"BATCH_SIZE\"],\n",
    "        num_workers=config[\"NUM_WORKERS\"],\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        batch_size=config[\"BATCH_SIZE\"],\n",
    "        num_workers=config[\"NUM_WORKERS\"],\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    elapsed_time = time() - start_time\n",
    "    logging.info(f\"Time taken to create data loaders: {elapsed_time:.2f} seconds\")\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "# Fetches tokenizer relevant to the model\n",
    "def create_or_load_tokenizer(checkpointed_path=None):\n",
    "    if checkpointed_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpointed_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config[\"MODEL_NAME\"], cache_dir=f\"{ROOT_PATH}/datasets\"\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "def run_data_pipeline(tokenizer, load_from_file=False):\n",
    "    # Measure how much RAM is being used before anything runs\n",
    "    ram_usage = get_ram_usage()\n",
    "    logging.info(f\"Baseline: RAM used: {ram_usage:.2f} MB\")\n",
    "\n",
    "    # Load data, either from url or from datasets folder\n",
    "    data_file_url = f\"{config['DATASET_URL']}/{config['DATASET_NAME']}.jsonl.zst\"\n",
    "    try:\n",
    "        if load_from_file:\n",
    "            raise Exception\n",
    "        dataset = load_dataset(\n",
    "            \"json\",\n",
    "            data_files=data_file_url,\n",
    "            num_proc=config[\"NUM_WORKERS\"],\n",
    "            split=\"train\",\n",
    "            cache_dir=f\"{ROOT_PATH}/datasets\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        dataset = load_dataset(\n",
    "            \"json\",\n",
    "            data_files=f\"{ROOT_PATH}/datasets/{config['DATASET_NAME']}.jsonl.zst\",\n",
    "            num_proc=config[\"NUM_WORKERS\"],\n",
    "            split=\"train\",\n",
    "            cache_dir=f\"{ROOT_PATH}/datasets\",\n",
    "        )\n",
    "\n",
    "    # Measurements relevant to the dataset\n",
    "    ram_usage = get_ram_usage()\n",
    "    logging.info(f\"RAM used: {ram_usage:.2f} MB\")\n",
    "    logging.info(f\"Dataset sample: {dataset[10]}\")\n",
    "    size_gb = dataset.dataset_size / (1024**3)\n",
    "    logging.info(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n",
    "\n",
    "    # Fetch a tokenizer and tokenize + split the dataset\n",
    "    train_dataset, valid_dataset = preprocess_data(dataset, tokenizer)\n",
    "\n",
    "    # Create a data collator and use it to make data loaders\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        train_dataset, valid_dataset, data_collator\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"TRAIN_DATASET\": train_dataset,\n",
    "        \"VALIDATION_DATASET\": valid_dataset,\n",
    "        \"TRAIN_DATALOADER\": train_dataloader,\n",
    "        \"VALIDATION_DATALOADER\": valid_dataloader,\n",
    "        \"TOKENIZER\": tokenizer,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create model\n",
    "def create_or_load_model(checkpointed_path=None, quantized=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if checkpointed_path:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpointed_path)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        configuration = AutoConfig.from_pretrained(config[\"MODEL_NAME\"])\n",
    "        if quantized:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                config[\"MODEL_NAME\"],\n",
    "                config=configuration,\n",
    "                device_map=\"auto\",\n",
    "                load_in_8bit=True,\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                config[\"MODEL_NAME\"], config=configuration\n",
    "            )\n",
    "            model.to(device)\n",
    "\n",
    "    # Measurements relevant to the model\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Model: {config['MODEL_NAME']}\")\n",
    "    logger.info(f\"Total Parameters: {total_params / 1e6:.2f} million\")\n",
    "    logger.info(f\"Trainable Parameters: {trainable_params / 1e6:.2f} million\")\n",
    "    logger.info(f\"Memory Footprint: {model.get_memory_footprint() / 1e6:,} MB\")\n",
    "    logger.info(f\"Model is on device: {model.device}\")\n",
    "    return model, device\n",
    "\n",
    "\n",
    "# Use the model to generate text\n",
    "def inference(model, tokenizer, device, max_tokens=128):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Generate a sequence of text tokens\n",
    "    inputs = tokenizer.encode(tokenizer.eos_token + \"This\", return_tensors=\"pt\").to(\n",
    "        device\n",
    "    )\n",
    "    output_sequence = model.generate(\n",
    "        inputs,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        max_length=max_tokens,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        use_cache=False,\n",
    "    )\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    generated_text = (\n",
    "        tokenizer.decode(output_sequence[0], clean_up_tokenization_spaces=True)\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .replace(\"\\t\", \" \")\n",
    "    )\n",
    "\n",
    "    # Put the model back into train mode\n",
    "    model.train()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Evaluate the model on a data loader\n",
    "def validate(model, device, valid_dataloader):\n",
    "    #     counter = 0\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        #         counter+=1\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "    #         if counter==10:\n",
    "    #             break\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(valid_dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_eval_loss)).item()\n",
    "    model.train()\n",
    "    return avg_eval_loss, perplexity\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train(model, device, data_dict):\n",
    "    model_save_path = f\"{ROOT_PATH}/models/{ENV}/fine_tuned_{config['MODEL_NAME']}_{config['DATASET_NAME']}_{config['TOKENIZED_NAME']}\"\n",
    "\n",
    "    # Setup logging\n",
    "    log_save_path = f\"{ROOT_PATH}/logs/{ENV}/{config['MODEL_NAME']}_{config['DATASET_NAME']}_{config['TOKENIZED_NAME']}\"\n",
    "    if not os.path.exists(log_save_path):\n",
    "        os.makedirs(log_save_path)\n",
    "    with open(f\"{log_save_path}/training.log\", \"w+\") as f:\n",
    "        f.write(\"epoch\\tbatch\\ttrain\\tloss\\tgenerated_text\\n\")\n",
    "    with open(f\"{log_save_path}/validation.log\", \"w+\") as f:\n",
    "        f.write(\"epoch\\tbatch\\tvalidation_loss\\tperplexity\\n\")\n",
    "\n",
    "    train_dataloader = data_dict[\"TRAIN_DATALOADER\"]\n",
    "    valid_dataloader = data_dict[\"VALIDATION_DATALOADER\"]\n",
    "    tokenizer = data_dict[\"TOKENIZER\"]\n",
    "\n",
    "    engine, optimizer, _, _ = deepspeed.initialize(\n",
    "        config=deepspeed_config,\n",
    "        model=model,\n",
    "        model_parameters=model.parameters(),\n",
    "        dist_init_required=False,\n",
    "    )\n",
    "\n",
    "    #     optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=config[\"LEARNING_RATE\"])\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=config[\"LEARNING_RATE\"],eps=config[\"EPSILON\"])\n",
    "    #     lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "    #                                                    num_warmup_steps=100,\n",
    "    #                                                    num_training_steps=config[\"NUM_EPOCHS\"] * len(train_dataloader))\n",
    "\n",
    "    model.train()\n",
    "    #     scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Go through each epoch\n",
    "    for epoch in tqdm(range(1, config[\"NUM_EPOCHS\"] + 1)):\n",
    "        counter = 0\n",
    "        running_loss = 0.0\n",
    "        logging.info(f\"Epoch: {epoch}/{config['NUM_EPOCHS']}\")\n",
    "\n",
    "        # Go through each batch in the data loader\n",
    "        for index, batch in tqdm(\n",
    "            enumerate(train_dataloader), total=len(train_dataloader)\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            if counter < 5:\n",
    "                print_gpu_utilization()\n",
    "                counter += 1\n",
    "            #             !echo \"GPU Memory: $(nvidia-smi | grep MiB |  awk '{print $9 $10 $11}')\"\n",
    "            # Measure average loss and sample an output from the model, at each sampling interval\n",
    "            if (index + 1) % config[\"SAMPLING_INTERVAL\"] == 0:\n",
    "                avg_loss = running_loss / config[\"SAMPLING_INTERVAL\"]\n",
    "                logging.info(\n",
    "                    f\"Batch {index+1}/{len(train_dataloader)}, Loss: {avg_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "                #                 generated_text = inference(model, tokenizer, device)\n",
    "                #                 logging.info(f\"Text:\\n{generated_text}\")\n",
    "\n",
    "                with open(f\"{log_save_path}/training.log\", \"a\") as f:\n",
    "                    f.write(f\"{epoch}\\t{index+1}\\t{avg_loss}\\t{generated_text}\\n\")\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Save the model at each checkpointing interval\n",
    "            if (index + 1) % config[\"CHECKPOINTING_INTERVAL\"] == 0:\n",
    "                logging.info(\n",
    "                    f\"Checkpointing model at epoch={epoch} and batch={index+1}\\n\"\n",
    "                )\n",
    "\n",
    "                checkpointing_path = f\"{model_save_path}_{epoch}_{index}\"\n",
    "                model.save_pretrained(checkpointing_path)\n",
    "                tokenizer.save_pretrained(checkpointing_path)\n",
    "\n",
    "            # Validate the model at each validation interval\n",
    "            if (index + 1) % config[\"VALIDATION_INTERVAL\"] == 0:\n",
    "                logging.info(\"Running Validation...\")\n",
    "                avg_eval_loss, perplexity = validate(model, device, valid_dataloader)\n",
    "                logging.info(\n",
    "                    f\"Batch {index + 1}/{len(train_dataloader)}, Validation Loss: {avg_eval_loss:.4f}, Perplexity: {perplexity:.2f}\"\n",
    "                )\n",
    "                with open(f\"{log_save_path}/validation.log\", \"a\") as f:\n",
    "                    f.write(f\"{epoch}\\t{index+1}\\t{avg_eval_loss}\\t{perplexity}\\n\")\n",
    "\n",
    "            # Backpropagation\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss = engine(**batch)\n",
    "            engine.backward(loss)\n",
    "            engine.step()\n",
    "    #             with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    #                 outputs = model(**batch)\n",
    "    #                 loss = outputs.loss\n",
    "    #                 loss = loss / config[\"GRADIENT_ACCUMULATION_STEPS\"]\n",
    "\n",
    "    # Using a scaler\n",
    "    #             scaler.scale(loss).backward()\n",
    "    #             loss.backward()\n",
    "\n",
    "    #             running_loss += loss.item()*config[\"GRADIENT_ACCUMULATION_STEPS\"]\n",
    "\n",
    "    #             if (index+1) % config[\"GRADIENT_ACCUMULATION_STEPS\"] == 0:\n",
    "    #                 scaler.step(optimizer)\n",
    "    #                 scaler.update()\n",
    "    #                 lr_scheduler.step()\n",
    "    #                 optimizer.step()\n",
    "\n",
    "    # After all epochs are completed, save the final model and tokenier\n",
    "    #     model.save_pretrained(model_save_path)\n",
    "    engine.save_checkpoint(model_save_path, model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"RANK\"] = \"0\"\n",
    "    os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "    torch.cuda.set_device(0)  # Set the GPU device you'd like to use\n",
    "    deepspeed.init_distributed(\n",
    "        dist_backend=\"nccl\", rank=0, world_size=1\n",
    "    )  # Initialize the distributed backend\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    #     checkpointed_path = f\"/scratch/{NET_ID}/finetunedgpt2_5epochs\"\n",
    "    checkpointed_path = None\n",
    "    dschf = HfDeepSpeedConfig(deepspeed_config)\n",
    "    tokenizer = create_or_load_tokenizer(checkpointed_path=checkpointed_path)\n",
    "    data_dict = run_data_pipeline(tokenizer, load_from_file=False)\n",
    "    model, device = create_or_load_model(checkpointed_path=checkpointed_path)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    #     generated_text = inference(model, tokenizer, device)\n",
    "    #     logging.info(f\"Initial Text:\\n{generated_text}\")\n",
    "\n",
    "    train(model, device, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
