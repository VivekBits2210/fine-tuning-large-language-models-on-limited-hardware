{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Hostname: gr057.hpc.nyu.edu\n",
      "Processor: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz\n",
      "RAM: 187Gi\n"
     ]
    }
   ],
   "source": [
    "!module load cuda/11.6.2\n",
    "!module load cudnn/8.6.0.163-cuda11\n",
    "!pip install -q --use-feature=2020-resolver pynvml zstandard datasets psutil transformers torch bitsandbytes accelerate loralib peft\n",
    "!echo \"Hostname: $(hostname)\"\n",
    "!echo \"Processor: $(lscpu | grep 'Model name' | awk -F ':' '{print $2}' | xargs)\"\n",
    "!echo \"RAM: $(free -h | grep 'Mem:' | awk '{print $4}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Quadro RTX 8000\n",
      "GPU Memory: 0MiB/46080MiB\n"
     ]
    }
   ],
   "source": [
    "!echo \"GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)\"\n",
    "!echo \"GPU Memory: $(nvidia-smi | grep MiB |  awk '{print $9 $10 $11}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 06:31:28,128 - INFO - <module>:90 - Is Flash Attention Enabled: True\n",
      "2023-05-01 06:31:28,128 - INFO - <module>:91 - Is Mem Efficient SDP Enabled: False\n",
      "2023-05-01 06:31:28,129 - INFO - <module>:92 - Is Math SDP Enabled: False\n",
      "2023-05-01 06:31:28,131 - INFO - print_gpu_utilization:102 - GPU memory occupied: 4314 MB.\n",
      "loading configuration file config.json from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/vocab.json\n",
      "loading file merges.txt from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /scratch/vgn2004/fine_tuning/datasets/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "2023-05-01 06:31:28,286 - INFO - run_data_pipeline:219 - Baseline: RAM used: 2053.76 MB\n",
      "2023-05-01 06:31:28,286 - ERROR - run_data_pipeline:232 - \n",
      "2023-05-01 06:31:28,355 - WARNING - download_and_prepare:817 - Found cached dataset json (/scratch/vgn2004/fine_tuning/datasets/json/default-f34239bbfb70cca2/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "2023-05-01 06:31:28,369 - INFO - run_data_pipeline:241 - RAM used: 2058.04 MB\n",
      "2023-05-01 06:31:28,369 - INFO - run_data_pipeline:242 - Dataset sample: {'meta': {'APPLICATION_ID': 100075}, 'text': \"ACF's Office of Refugee Resettlement (ORR) administers a variety of social service programs intended to connect newly resettled refugees with critical resources, help them become economically self-sufficient, and help them integrate into American society. One such program is the Refugee Cash Assistance (RCA) program, which provides both financial support and social services to newly resettled refugees. Refugee Cash Assistance is similar to TANF in that both are cash assistance programs that provide services aimed at promoting self-sufficiency; however the content, mode of delivery and rules surrounding these services vary significantly by state and locality. Some counties and states have reportedly integrated the delivery of TANF and RCA in a purposeful way to better serve refugees. However, there is little documented information on the extent to which refugees access benefits and services through TANF and RCA, differences in refugee characteristics between the two programs, how outcomes compare for refugees served under these two programs, whether integration of these programs holds promise for refugee self-sufficiency, and whether data is available to answer these questions. The Understanding the Intersection Between TANF and Refugee Cash Assistance Services project aims to improve understanding of how RCA and TANF serve refugee populations, how these programs intersect, and how these programs may be related to refugee self-sufficiency and employment outcomes. In fall 2014 ACF launched this descriptive study to document the similarities and differences between cash assistance and associated social services offered under RCA and TANF across different selected jurisdictions. The study aims to better understand the population of refugees served by TANF and RCA, and the major differences in programmatic services associated with these two programs. The study will also explore how states and localities have coordinated TANF and RCA programs to deliver social services to refugees and whether these approaches hold promise for long-term job stability and economic self-sufficiency among refugees. This field study will provide a deeper understanding of current social service delivery systems serving refugees and will help to identify gaps in existing knowledge and data around these systems. By improving knowledge of these programs and participant experiences, ACF hopes to move toward better serving this population. The project is being conducted by Abt Associates and MEF Associates.\"}\n",
      "2023-05-01 06:31:28,370 - INFO - run_data_pipeline:244 - Dataset size (cache file) : 2.01 GB\n",
      "2023-05-01 06:31:28,370 - INFO - preprocess_data:138 - Loading dataset from disk...\n",
      "2023-05-01 06:31:28,383 - INFO - preprocess_data:143 - Time taken to load dataset from : 0.01 seconds\n",
      "2023-05-01 06:31:28,384 - INFO - create_dataloaders:187 - Creating data loaders...\n",
      "2023-05-01 06:31:28,384 - INFO - create_dataloaders:202 - Time taken to create data loaders: 0.00 seconds\n",
      "loading configuration file config.json from cache at /home/vgn2004/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "loading weights file pytorch_model.bin from cache at /home/vgn2004/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/pytorch_model.bin\n",
      "Instantiating OPTForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Detected 8-bit loading: activating 8-bit loading for this model\n",
      "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
      "\n",
      "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-350m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/vgn2004/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/cb32f77e905cccbca1d970436fb0f5e6b58ee3c5/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "2023-05-01 06:31:30,400 - INFO - create_or_load_model:293 - Model: facebook/opt-350m\n",
      "2023-05-01 06:31:30,402 - INFO - print_trainable_parameters:118 - Parameters: Trainable- 0.00M|| All- 331.20M || Trainable%- 0.0\n",
      "2023-05-01 06:31:30,404 - INFO - create_or_load_model:295 - Memory Memory Footprint: 359.993344 MB\n",
      "2023-05-01 06:31:30,404 - INFO - create_or_load_model:296 - Model is on device: cuda:0\n",
      "2023-05-01 06:31:30,444 - INFO - <module>:371 - Peft Model: facebook/opt-350m\n",
      "2023-05-01 06:31:30,446 - INFO - print_trainable_parameters:118 - Parameters: Trainable- 1.57M|| All- 332.77M || Trainable%- 0.472659014678278\n",
      "2023-05-01 06:31:30,449 - INFO - <module>:373 - Memory Memory Footprint: 366.2848 MB\n",
      "2023-05-01 06:31:30,449 - INFO - <module>:374 - Model is on device: cuda\n",
      "2023-05-01 06:31:30,450 - INFO - <module>:375 - Iterations: 1663290\n",
      "PyTorch: setting up devices\n",
      "The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 887,085\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 415,800\n",
      "  Number of trainable parameters = 1,572,864\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='415800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    19/415800 00:41 < 281:39:01, 0.41 it/s, Epoch 0.00/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 06:31:37,984 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5196 MB.\n",
      "2023-05-01 06:31:38,142 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5196 MB.\n",
      "2023-05-01 06:31:47,995 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5198 MB.\n",
      "2023-05-01 06:31:48,152 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5198 MB.\n",
      "2023-05-01 06:31:58,007 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5198 MB.\n",
      "2023-05-01 06:31:58,179 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5198 MB.\n",
      "2023-05-01 06:32:08,018 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5198 MB.\n",
      "2023-05-01 06:32:08,191 - INFO - print_gpu_utilization:102 - GPU memory occupied: 5198 MB.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3fe92a405d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;31m#Train the model using the Trainer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m         )\n\u001b[0;32m-> 1662\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1663\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_grad_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Change net ID here to use your scratch folder\n",
    "ENV = \"dev\"\n",
    "NET_ID = \"vgn2004\"\n",
    "DATA_PATH =  f\"/scratch/{NET_ID}/fine_tuning\" \n",
    "ROOT_PATH = f\"/scratch/{NET_ID}/fine_tuning/{ENV}\"\n",
    "\n",
    "# Ensure that packages can be found\n",
    "import sys\n",
    "sys.path.insert(0, f\"/home/{NET_ID}/.local/lib/python3.8/site-packages\")\n",
    "\n",
    "\n",
    "# Global configurations\n",
    "config = {\n",
    "    \"DATASET_URL\": \"https://the-eye.eu/public/AI/pile_v2/data\",\n",
    "    \"DATASET_NAME\": \"NIH_ExPORTER_awarded_grant_text\",\n",
    "    \"NUM_WORKERS\": 8,\n",
    "    \"DATASET_SPLIT_RATIO\": 0.9,\n",
    "    \"PADDING_STRATEGY\": \"max_length\",\n",
    "    \"MAX_TOKENS\": 128,\n",
    "    \"MODEL_NAME\": \"facebook/opt-350m\",\n",
    "    \"TOKENIZED_NAME\": \"opt_350\",\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"NUM_EPOCHS\": 30,\n",
    "    \"LEARNING_RATE\": 5e-4,\n",
    "    \"MIN_LEARNING_RATE\": 5e-5,\n",
    "    \"EPSILON\": 1e-8,\n",
    "    \"BETAS\": (0.9,0.95),\n",
    "    \"GRADIENT_CLIP\": 1.0,\n",
    "    \"WEIGHT_DECAY\": 0.01,\n",
    "    \"DECAY_STYLE\": \"cosine\", #not used currently\n",
    "    \"WARMUP_RATIO\": 0.003,\n",
    "    \"SAMPLING_INTERVAL\": 20,\n",
    "    \"CHECKPOINTING_INTERVAL\": 100,\n",
    "    \"VALIDATION_INTERVAL\": 500,\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
    "\n",
    "}\n",
    "\n",
    "# Low Rank Adapters config\n",
    "from peft import LoraConfig, PeftConfig, get_peft_model \n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Weight Quantization config\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_skip_modules=[\"lm_head\"],\n",
    "    llm_int8_threshold=3.0\n",
    ")\n",
    "\n",
    "# Ensure that GPU can be found\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "# os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "# Setup logging\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s')\n",
    "\n",
    "# Packages for profiling\n",
    "import random\n",
    "import psutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "from pynvml import *\n",
    "\n",
    "# Packages for data loading\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Core packages\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "logging.info(f\"Is Flash Attention Enabled: {torch.backends.cuda.flash_sdp_enabled()}\")\n",
    "logging.info(f\"Is Mem Efficient SDP Enabled: {torch.backends.cuda.mem_efficient_sdp_enabled()}\")\n",
    "logging.info(f\"Is Math SDP Enabled: {torch.backends.cuda.math_sdp_enabled()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Get GPU Utilization\n",
    "def print_gpu_utilization():\n",
    "    while True:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        logger.info(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "\n",
    "# Returns RAM usage in MB\n",
    "def get_ram_usage():\n",
    "    return psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# Returns number of trainable parameters and percentage\n",
    "def print_trainable_parameters(model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        logger.info(\n",
    "            f\"Parameters: Trainable- {trainable_params/1e6:.2f}M|| All- {all_param/1e6:.2f}M || Trainable%- {100 * trainable_params / all_param}\"\n",
    "        )\n",
    "\n",
    "#Takes a batch of inputs and runs the tokenizer on them\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=config[\"PADDING_STRATEGY\"],\n",
    "        truncation=True,\n",
    "        max_length=config[\"MAX_TOKENS\"],\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "# Tokenizes dataset and creates train and validation split\n",
    "def preprocess_data(dataset, tokenizer):\n",
    "    tokenized_dataset_path = f\"{DATA_PATH}/datasets/tokenized_{config['DATASET_NAME']}_{config['TOKENIZED_NAME']}\"\n",
    "    train_dataset_path = f\"{tokenized_dataset_path}_train\"\n",
    "    valid_dataset_path = f\"{tokenized_dataset_path}_valid\"\n",
    "    if os.path.exists(train_dataset_path) and os.path.exists(valid_dataset_path):\n",
    "        logger.info(f\"Loading dataset from disk...\")\n",
    "        start_time = time.time()\n",
    "        train_dataset = load_from_disk(train_dataset_path)\n",
    "        valid_dataset = load_from_disk(valid_dataset_path)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Time taken to load dataset from : {elapsed_time:.2f} seconds\")\n",
    "        return train_dataset, valid_dataset\n",
    "        \n",
    "    logger.info(f\"Tokenizing the dataset...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        tokenized_dataset = load_from_disk(tokenized_dataset_path)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            fn_kwargs={'tokenizer': tokenizer},\n",
    "            batched=True,\n",
    "            num_proc=8,\n",
    "            remove_columns=[\"text\", \"meta\"],\n",
    "        )\n",
    "        tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"Time taken to tokenize the dataset: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    logger.info(f\"Splitting the dataset...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if os.path.exists(train_dataset_path) and os.path.exists(valid_dataset_path):\n",
    "        train_dataset = load_from_disk(train_dataset_path)\n",
    "        valid_dataset = load_from_disk(valid_dataset_path)\n",
    "    else:\n",
    "        train_size = int(config[\"DATASET_SPLIT_RATIO\"] * len(tokenized_dataset))\n",
    "        datasets = DatasetDict({\n",
    "            'train': Dataset.from_dict(tokenized_dataset[:train_size]),\n",
    "            'valid': Dataset.from_dict(tokenized_dataset[train_size:])\n",
    "        })\n",
    "        train_dataset = datasets['train']\n",
    "        valid_dataset = datasets['valid']\n",
    "        train_dataset.save_to_disk(train_dataset_path)\n",
    "        valid_dataset.save_to_disk(valid_dataset_path)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"Time taken to split the datasets (or load pre-split datasets): {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "# Creates data loaders\n",
    "def create_dataloaders(train_dataset, valid_dataset, data_collator):\n",
    "    logger.info(f\"Creating data loaders...\")\n",
    "    start_time = time.time()\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=RandomSampler(train_dataset),\n",
    "                                  batch_size=config[\"BATCH_SIZE\"],\n",
    "                                  num_workers=config[\"NUM_WORKERS\"],\n",
    "                                  collate_fn=data_collator,\n",
    "                                  pin_memory=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                                  sampler=SequentialSampler(valid_dataset),\n",
    "                                  batch_size=config[\"BATCH_SIZE\"],\n",
    "                                  num_workers=config[\"NUM_WORKERS\"],\n",
    "                                  collate_fn=data_collator,\n",
    "                                  pin_memory=True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time taken to create data loaders: {elapsed_time:.2f} seconds\")\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "# Fetches tokenizer relevant to the model\n",
    "def create_or_load_tokenizer(checkpointed_path=None):\n",
    "    if checkpointed_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpointed_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config[\"MODEL_NAME\"], cache_dir=f\"{DATA_PATH}/datasets\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = 'left'\n",
    "    return tokenizer\n",
    "\n",
    "# Data preparation\n",
    "def run_data_pipeline(tokenizer, load_from_file=False):\n",
    "    # Measure how much RAM is being used before anything runs\n",
    "    ram_usage = get_ram_usage()\n",
    "    logging.info(f\"Baseline: RAM used: {ram_usage:.2f} MB\")\n",
    "\n",
    "    # Load data, either from url or from datasets folder\n",
    "    data_file_url = f\"{config['DATASET_URL']}/{config['DATASET_NAME']}.jsonl.zst\"\n",
    "    try:\n",
    "        if load_from_file:\n",
    "            raise Exception\n",
    "        dataset = load_dataset(\"json\",\n",
    "                               data_files=data_file_url,\n",
    "                               num_proc=config[\"NUM_WORKERS\"],\n",
    "                               split=\"train\",\n",
    "                               cache_dir=f\"{DATA_PATH}/datasets\")\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        dataset = load_dataset(\"json\",\n",
    "                               data_files=f\"{DATA_PATH}/datasets/{config['DATASET_NAME']}.jsonl.zst\",\n",
    "                               num_proc=config[\"NUM_WORKERS\"],\n",
    "                               split=\"train\",\n",
    "                               cache_dir=f\"{DATA_PATH}/datasets\")\n",
    "\n",
    "    # Measurements relevant to the dataset\n",
    "    ram_usage = get_ram_usage()\n",
    "    logging.info(f\"RAM used: {ram_usage:.2f} MB\")\n",
    "    logging.info(f\"Dataset sample: {dataset[10]}\")\n",
    "    size_gb = dataset.dataset_size / (1024 ** 3)\n",
    "    logging.info(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n",
    "\n",
    "    # Fetch a tokenizer and tokenize + split the dataset\n",
    "    train_dataset, valid_dataset = preprocess_data(dataset, tokenizer)\n",
    "\n",
    "    # Create a data collator and use it to make data loaders\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(train_dataset, valid_dataset, data_collator)\n",
    "\n",
    "    return {\n",
    "        \"TRAIN_DATASET\": train_dataset,\n",
    "        \"VALIDATION_DATASET\": valid_dataset,\n",
    "        \"TRAIN_DATALOADER\": train_dataloader,\n",
    "        \"VALIDATION_DATALOADER\": valid_dataloader,\n",
    "        \"TOKENIZER\": tokenizer,\n",
    "        \"DATA_COLLATOR\": data_collator\n",
    "    }\n",
    "\n",
    "\n",
    "# Create model\n",
    "def create_or_load_model(checkpointed_path=None, frozen=True, cast_layer_norm_to_fp32=True, cast_output_to_fp32=True):\n",
    "    class CastOutputToFloat(torch.nn.Sequential):\n",
    "        def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if checkpointed_path:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpointed_path)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        configuration = AutoConfig.from_pretrained(config[\"MODEL_NAME\"])\n",
    "        model = AutoModelForCausalLM.from_pretrained(config[\"MODEL_NAME\"], config=configuration, load_in_8bit=True, device_map='auto', quantization_config=quantization_config)\n",
    "\n",
    "        if frozen:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        if cast_layer_norm_to_fp32:\n",
    "            for param in model.parameters():\n",
    "                if param.ndim == 1:\n",
    "                    param.data = param.data.to(torch.float32)\n",
    "                \n",
    "    #Enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    \n",
    "    if cast_output_to_fp32:\n",
    "        model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "    \n",
    "    # Log details\n",
    "    logger.info(f\"Model: {config['MODEL_NAME']}\")\n",
    "    print_trainable_parameters(model)\n",
    "    logger.info(f\"Memory Memory Footprint: {model.get_memory_footprint() / 1e6:,} MB\")\n",
    "    logger.info(f\"Model is on device: {model.device}\")\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    return model, device\n",
    "\n",
    "# Use the model to generate text\n",
    "def generate(model, inputs):\n",
    "    output_sequence = model.generate(\n",
    "        **inputs,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        max_length=config[\"MAX_TOKENS\"],\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return output_sequence\n",
    "    \n",
    "def inference(model, tokenizer, device, max_tokens=128):\n",
    "    # Put the model in eval mode and enable caching\n",
    "    model.config.use_cache = True\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(tokenizer.eos_token+\"This\", return_tensors=\"pt\").to(device)\n",
    "    # Generate a sequence of text tokens\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output_sequence = generate(model, inputs)\n",
    "        \n",
    "\n",
    "    # Decode the tokens to text\n",
    "    generated_text = tokenizer.decode(output_sequence[0], \n",
    "                                      clean_up_tokenization_spaces=True,\n",
    "                                      skip_special_tokens=True).replace('\\n', '').replace('\\t', ' ')\n",
    "\n",
    "    # Put the model back into train mode and disable caching\n",
    "    model.train()\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "    \n",
    "# Custom training function\n",
    "def custom_training_function(trainer, model, inputs):\n",
    "    model.train()\n",
    "    inputs = {k: v.to(trainer.args.device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Custom evaluation function\n",
    "def custom_evaluation_function(trainer, model, inputs):\n",
    "    model.eval()\n",
    "    inputs = {k: v.to(trainer.args.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # start GPU utilization monitoring\n",
    "    import threading\n",
    "    gpu_monitor_thread = threading.Thread(target=print_gpu_utilization)\n",
    "    gpu_monitor_thread.start()\n",
    "    \n",
    "    checkpointed_path = None\n",
    "    tokenizer = create_or_load_tokenizer(checkpointed_path=checkpointed_path)\n",
    "    data_dict = run_data_pipeline(tokenizer, load_from_file=True)\n",
    "\n",
    "    model, device = create_or_load_model(checkpointed_path=checkpointed_path, \n",
    "                                         frozen=True,\n",
    "                                         cast_layer_norm_to_fp32=True,\n",
    "                                         cast_output_to_fp32=True)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    logger.info(f\"Peft Model: {config['MODEL_NAME']}\")\n",
    "    print_trainable_parameters(model)\n",
    "    logger.info(f\"Memory Memory Footprint: {model.get_memory_footprint() / 1e6:,} MB\")\n",
    "    logger.info(f\"Model is on device: {model.device}\")\n",
    "    logger.info(f\"Iterations: {len(data_dict['TRAIN_DATALOADER'])*config['NUM_EPOCHS']}\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=f\"{ROOT_PATH}/models\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=config[\"NUM_EPOCHS\"],\n",
    "    per_device_train_batch_size=config[\"BATCH_SIZE\"],\n",
    "    per_device_eval_batch_size=config[\"BATCH_SIZE\"],\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=config[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
    "    learning_rate=config[\"LEARNING_RATE\"],\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.05,\n",
    "    log_level=\"info\",\n",
    "    logging_dir=f\"{ROOT_PATH}/logs\",\n",
    "    logging_steps=config[\"SAMPLING_INTERVAL\"],\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    dataloader_num_workers=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=config[\"CHECKPOINTING_INTERVAL\"],\n",
    "    eval_steps=config[\"VALIDATION_INTERVAL\"],\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_drop_last=True\n",
    "    )\n",
    "    \n",
    "#     training_args = training_args.set_optimizer(name=\"adafactor\", lr=config[\"LEARNING_RATE\"], scale_parameter=False, relative_step=False, warmup_init=False)\n",
    "\n",
    "    # Create the Trainer object\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_dict[\"TRAIN_DATALOADER\"].dataset,\n",
    "        eval_dataset=data_dict[\"VALIDATION_DATALOADER\"].dataset,\n",
    "        data_collator=data_dict[\"DATA_COLLATOR\"],\n",
    "        compute_metrics=None\n",
    "    )\n",
    "\n",
    "    #Train the model using the Trainer object\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of training iterations\n",
    "# lr_decay_iters = max_iters # should be ~= max_iters per Chinchilla\n",
    "# grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# warmup_iters = 0.003*max_iters # how many steps to warm up for\n",
    "\n",
    "\n",
    "\n",
    "#         # Create the TrainingArguments object\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=f\"{ROOT_PATH}/models\",\n",
    "#             overwrite_output_dir=True,\n",
    "#             num_train_epochs=config[\"NUM_EPOCHS\"],\n",
    "#             per_device_train_batch_size=config[\"BATCH_SIZE\"],\n",
    "#             per_device_eval_batch_size=config[\"BATCH_SIZE\"],\n",
    "#             gradient_checkpointing=True,\n",
    "#             auto_find_batch_size=True,\n",
    "#             gradient_accumulation_steps=config[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
    "#             learning_rate=config[\"LEARNING_RATE\"],\n",
    "#             weight_decay=0.01,\n",
    "#             fp16=True,\n",
    "#             fp16_full_eval=True,\n",
    "#             warmup_ratio=0.05,\n",
    "#             log_level=\"info\",\n",
    "#             logging_dir=f\"{ROOT_PATH}/logs\",\n",
    "#             logging_steps=config[\"SAMPLING_INTERVAL\"],\n",
    "#             report_to=\"none\",\n",
    "#             disable_tqdm=False,\n",
    "#             dataloader_num_workers=8,\n",
    "#             evaluation_strategy=\"steps\",\n",
    "#             save_steps=config[\"CHECKPOINTING_INTERVAL\"],\n",
    "#             eval_steps=config[\"VALIDATION_INTERVAL\"],\n",
    "#             prediction_loss_only=True,\n",
    "#             dataloader_drop_last=True,\n",
    "#         )\n",
    "#         training_args = training_args.set_optimizer(name=\"adafactor\", lr=config[\"LEARNING_RATE\"], scale_parameter=False, relative_step=False, warmup_init=False)\n",
    "\n",
    "#         # Create the Trainer object\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=data_dict[\"TRAIN_DATALOADER\"].dataset,\n",
    "#             eval_dataset=data_dict[\"VALIDATION_DATALOADER\"].dataset,\n",
    "#             data_collator=data_dict[\"DATA_COLLATOR\"],\n",
    "#             compute_loss=custom_training_function,\n",
    "#             compute_metrics=None,\n",
    "#         )\n",
    "\n",
    "        # Train the model using the Trainer object\n",
    "#         trainer.train()\n",
    "#     elif config[\"PEFT\"]:\n",
    "#         model, device = create_or_load_model(checkpointed_path=checkpointed_path, cast_layer_norm_to_fp32=True)\n",
    "        \n",
    "#         # Model compilation\n",
    "#         # model = torch.compile(model)\n",
    "        \n",
    "#         generated_text = inference(model, tokenizer, device)\n",
    "#         logging.info(f\"Initial Text:\\n{generated_text}\")\n",
    "\n",
    "#         train(model, device, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
