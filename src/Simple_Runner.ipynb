{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner which uses Trainer from transformers library\n",
    "\n",
    "Unlike the other code in this repo, we will use the Trainer and TrainingArguments class from HuggingFace. The goal is to compare performance with qlora_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: At 08:59 PM 10s, from <ipython-input-3-c200f21b3bd6>:<module>:58 => RAM Usage: 2050.92578125 MB\n",
      "INFO: At 08:59 PM 10s, from <ipython-input-3-c200f21b3bd6>:<module>:58 => RAM Usage: 2050.92578125 MB\n",
      "INFO: At 08:59 PM 10s, from <ipython-input-3-c200f21b3bd6>:<module>:58 => RAM Usage: 2050.92578125 MB\n",
      "INFO: At 08:59 PM 10s, from <ipython-input-3-c200f21b3bd6>:<module>:59 => GPU Utilization: 1022 MB\n",
      "INFO: At 08:59 PM 10s, from <ipython-input-3-c200f21b3bd6>:<module>:59 => GPU Utilization: 1022 MB\n",
      "INFO: At 08:59 PM 10s, from <ipython-input-3-c200f21b3bd6>:<module>:59 => GPU Utilization: 1022 MB\n",
      "INFO: At 08:59 PM 10s, from user_configuration.py:__init__:18 => The base directory is set to /scratch.\n",
      "INFO: At 08:59 PM 10s, from user_configuration.py:__init__:18 => The base directory is set to /scratch.\n",
      "INFO: At 08:59 PM 10s, from user_configuration.py:__init__:18 => The base directory is set to /scratch.\n",
      "INFO: At 08:59 PM 10s, from torch_configuration.py:commit:17 => Flash attention is enabled!\n",
      "INFO: At 08:59 PM 10s, from torch_configuration.py:commit:17 => Flash attention is enabled!\n",
      "INFO: At 08:59 PM 10s, from torch_configuration.py:commit:17 => Flash attention is enabled!\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing load_for_model...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing load_for_model...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing load_for_model...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => load_for_model executed in 0.1631 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => load_for_model executed in 0.1631 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => load_for_model executed in 0.1631 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing fetch_train_validation_split_from_disk...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing fetch_train_validation_split_from_disk...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing fetch_train_validation_split_from_disk...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => fetch_train_validation_split_from_disk executed in 0.0063 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => fetch_train_validation_split_from_disk executed in 0.0063 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => fetch_train_validation_split_from_disk executed in 0.0063 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing fetch_dataloaders...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing fetch_dataloaders...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing fetch_dataloaders...\n",
      "INFO: At 08:59 PM 10s, from data_manager.py:fetch_dataloaders:134 => Batch size is set to 64.\n",
      "INFO: At 08:59 PM 10s, from data_manager.py:fetch_dataloaders:134 => Batch size is set to 64.\n",
      "INFO: At 08:59 PM 10s, from data_manager.py:fetch_dataloaders:134 => Batch size is set to 64.\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => fetch_dataloaders executed in 0.0016 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => fetch_dataloaders executed in 0.0016 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:16 => fetch_dataloaders executed in 0.0016 seconds\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing load...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing load...\n",
      "INFO: At 08:59 PM 10s, from profiler_utils.py:wrapper:11 => Executing load...\n",
      "INFO: At 08:59 PM 10s, from model_manager.py:load:36 => Quantizing the model with config as BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": false,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "\n",
      "INFO: At 08:59 PM 10s, from model_manager.py:load:36 => Quantizing the model with config as BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": false,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "\n",
      "INFO: At 08:59 PM 10s, from model_manager.py:load:36 => Quantizing the model with config as BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": false,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "\n",
      "INFO: At 08:59 PM 11s, from modeling.py:get_balanced_memory:799 => We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO: At 08:59 PM 11s, from modeling.py:get_balanced_memory:799 => We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO: At 08:59 PM 11s, from modeling.py:get_balanced_memory:799 => We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:16 => load executed in 0.8080 seconds\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:16 => load executed in 0.8080 seconds\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:16 => load executed in 0.8080 seconds\n",
      "INFO: At 08:59 PM 11s, from <ipython-input-3-c200f21b3bd6>:<module>:120 => OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "INFO: At 08:59 PM 11s, from <ipython-input-3-c200f21b3bd6>:<module>:120 => OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: At 08:59 PM 11s, from <ipython-input-3-c200f21b3bd6>:<module>:120 => OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:11 => Executing encode...\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:11 => Executing encode...\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:11 => Executing encode...\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:16 => encode executed in 0.0020 seconds\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:16 => encode executed in 0.0020 seconds\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:16 => encode executed in 0.0020 seconds\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:11 => Executing infer...\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:11 => Executing infer...\n",
      "INFO: At 08:59 PM 11s, from profiler_utils.py:wrapper:11 => Executing infer...\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => infer executed in 1.7453 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => infer executed in 1.7453 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => infer executed in 1.7453 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:11 => Executing decode...\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:11 => Executing decode...\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:11 => Executing decode...\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => decode executed in 0.0017 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => decode executed in 0.0017 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => decode executed in 0.0017 seconds\n",
      "INFO: At 08:59 PM 13s, from <ipython-input-3-c200f21b3bd6>:<module>:127 => Generated Text Before Fine-Tuning:\n",
      "This just in.... I'm on Reddit right now.   I've already ordered a lot of this shit. I'm a massive conspiracy theorist so I'm all for conspiracy theories but it's ridiculous that there's still a problem. I'm still going to the beach today for the first time this summer, but for the past 2 years I've never been out of the area where most of the crime is. And it's so hard to put a price on the people's lives. It just sounds like people are too busy to be involved in the process.I'm currently looking to open a joint account with other\n",
      "INFO: At 08:59 PM 13s, from <ipython-input-3-c200f21b3bd6>:<module>:127 => Generated Text Before Fine-Tuning:\n",
      "This just in.... I'm on Reddit right now.   I've already ordered a lot of this shit. I'm a massive conspiracy theorist so I'm all for conspiracy theories but it's ridiculous that there's still a problem. I'm still going to the beach today for the first time this summer, but for the past 2 years I've never been out of the area where most of the crime is. And it's so hard to put a price on the people's lives. It just sounds like people are too busy to be involved in the process.I'm currently looking to open a joint account with other\n",
      "INFO: At 08:59 PM 13s, from <ipython-input-3-c200f21b3bd6>:<module>:127 => Generated Text Before Fine-Tuning:\n",
      "This just in.... I'm on Reddit right now.   I've already ordered a lot of this shit. I'm a massive conspiracy theorist so I'm all for conspiracy theories but it's ridiculous that there's still a problem. I'm still going to the beach today for the first time this summer, but for the past 2 years I've never been out of the area where most of the crime is. And it's so hard to put a price on the people's lives. It just sounds like people are too busy to be involved in the process.I'm currently looking to open a joint account with other\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:11 => Executing lorify...\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:11 => Executing lorify...\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:11 => Executing lorify...\n",
      "INFO: At 08:59 PM 13s, from model_manager.py:lorify:46 => Using LoRA with configuration: {'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='facebook/opt-125m', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=['q_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)}\n",
      "INFO: At 08:59 PM 13s, from model_manager.py:lorify:46 => Using LoRA with configuration: {'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='facebook/opt-125m', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=['q_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)}\n",
      "INFO: At 08:59 PM 13s, from model_manager.py:lorify:46 => Using LoRA with configuration: {'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='facebook/opt-125m', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=['q_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)}\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => lorify executed in 0.1253 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => lorify executed in 0.1253 seconds\n",
      "INFO: At 08:59 PM 13s, from profiler_utils.py:wrapper:16 => lorify executed in 0.1253 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='134' max='146300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   134/146300 00:50 < 15:37:17, 2.60 it/s, Epoch 0.05/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50: This study is the first to examine the role of the human immune system in the development of a novel immune system. The immune system is a complex system of cells and tissues that is composed of a number of different types of cells. The immune system is a complex system of cells and tissues that is composed of a number\n",
      "\n",
      "100: This study is the first of a series of studies to investigate the relationship between the development of the human immune system and the development of the immune system. The study will focus on the development of the immune system in the human body and the development of the immune system in the human body. The study will focus on the\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "\n",
    "from config import (\n",
    "    UserConfiguration,\n",
    "    LogConfiguration,\n",
    "    TorchConfiguration,\n",
    "    TokenizerConfiguration,\n",
    "    TextGenConfiguration,\n",
    "    SystemConfiguration,\n",
    "    TrainerConfiguration,\n",
    ")\n",
    "\n",
    "from os_environment_manager import OSEnvironmentManager\n",
    "from package_path_manager import PackagePathManager\n",
    "from model_manager import ModelManager\n",
    "from system_monitor import SystemMonitor\n",
    "\n",
    "from tokenization_manager import TokenizationManager\n",
    "from data_manager import DataManager\n",
    "\n",
    "# TODO: These should be picked up from command line\n",
    "from trainer import Trainer\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    IntervalStrategy,\n",
    ")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "NET_ID = \"vgn2004\"\n",
    "ENV = \"qlora\"\n",
    "NUM_WORKERS = 8\n",
    "MAX_TOKENS = 64\n",
    "MIN_GENERATION = 64\n",
    "MODEL_NAME = \"facebook/opt-125m\"\n",
    "DATASET_NAME = \"NIH_ExPORTER_awarded_grant_text\"\n",
    "TOKENIZER_NAME = \"speedup\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Constants\n",
    "OS_ENV_DICT = {\n",
    "    \"CUDA_VISIBLE_DEVICES\": 0,\n",
    "    \"TRANSFORMERS_NO_ADVISORY_WARNINGS\": \"true\",\n",
    "    \"TORCHDYNAMO_DISABLE\": 1,\n",
    "    \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear the GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Configure the logger, needed for initial utilization checks\n",
    "    LogConfiguration.setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Get initial RAM and GPU utilization\n",
    "    monitor = SystemMonitor()\n",
    "    logger.info(f\"RAM Usage: {monitor.get_ram_usage()} MB\")\n",
    "    logger.info(f\"GPU Utilization: {monitor.get_gpu_utilization()} MB\")\n",
    "\n",
    "    # Configurations\n",
    "\n",
    "    # Setup folder/file path related configurations\n",
    "    user_config = UserConfiguration(net_id=NET_ID, env=ENV)\n",
    "    system_config = SystemConfiguration(num_workers=NUM_WORKERS)\n",
    "    tokenizer_config = TokenizerConfiguration(\n",
    "        max_tokens=MAX_TOKENS, tokenizer_name=TOKENIZER_NAME\n",
    "    )\n",
    "    torch_config = TorchConfiguration()\n",
    "    torch_config.commit()\n",
    "\n",
    "    # System configurations\n",
    "\n",
    "    # Add Python packages to sys path\n",
    "    package_path_manager = PackagePathManager(user_config)\n",
    "    package_path_manager.add_package_paths_to_system()\n",
    "\n",
    "    # Add environment variables to OS env\n",
    "    os_env_manager = OSEnvironmentManager()\n",
    "    os_env_manager.update_from_dict(OS_ENV_DICT)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenization_manager = TokenizationManager(user_config, tokenizer_config)\n",
    "    tokenization_manager.load_for_model(MODEL_NAME)\n",
    "\n",
    "    # Datasets\n",
    "    data_manager = DataManager(user_config, system_config, tokenizer_config)\n",
    "    data_manager.dataset_name = DATASET_NAME\n",
    "    data_manager.set_data_collator(tokenization_manager.tokenizer)\n",
    "\n",
    "    # Tokenize dataset from scratch (skipped)\n",
    "    #     data_manager.create_dataset_from_jsonl_zst_file(name=DATASET_NAME,\n",
    "    #                                                     jsonl_zst_file_path=\"E:\\\\NIH_ExPORTER_awarded_grant_text.jsonl.zst\")\n",
    "    #     data_manager.create_tokenized_dataset()\n",
    "    #     training_dataset, validation_dataset = data_manager.fetch_train_validation_split()\n",
    "\n",
    "    # Load from disk\n",
    "    try:\n",
    "        (\n",
    "            training_dataset,\n",
    "            validation_dataset,\n",
    "        ) = data_manager.fetch_train_validation_split_from_disk()\n",
    "    except FileNotFoundError as fe:\n",
    "        logger.warning(f\"{fe.__repr__()}\")\n",
    "        data_manager.create_dataset_from_jsonl_zst_file(\n",
    "            name=DATASET_NAME,\n",
    "            jsonl_zst_file_path=\"/scratch/vgn2004/fine_tuning/datasets/NIH_ExPORTER_awarded_grant_text.jsonl.zst\",\n",
    "        )\n",
    "        data_manager.create_tokenized_dataset(tokenization_manager.tokenize)\n",
    "        (\n",
    "            training_dataset,\n",
    "            validation_dataset,\n",
    "        ) = data_manager.fetch_train_validation_split()\n",
    "\n",
    "    # Dataloaders\n",
    "    training_dataloader, validation_dataloader = data_manager.fetch_dataloaders(\n",
    "        training_dataset=training_dataset,\n",
    "        validation_dataset=validation_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model_manager = ModelManager(system_config)\n",
    "    model_manager.load(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    logger.info(model_manager.model)\n",
    "\n",
    "    # Text Generation\n",
    "    text_gen_config = TextGenConfiguration(\n",
    "        tokenization_manager.tokenizer, min_tokens_to_generate=MIN_GENERATION\n",
    "    )\n",
    "    prompt = tokenization_manager.encode(\"This\")\n",
    "    sequence = model_manager.infer(prompt, text_gen_config)\n",
    "    text = tokenization_manager.decode(sequence, text_gen_config)\n",
    "    logging.info(f\"Generated Text Before Fine-Tuning:\\n{text}\")\n",
    "\n",
    "    # Existing Trainer\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "\n",
    "    #     model_manager.model.gradient_checkpointing_enable()\n",
    "    model_manager.model = prepare_model_for_kbit_training(model_manager.model)\n",
    "    model_manager.lorify(\n",
    "        LoraConfig(\n",
    "            r=64, lora_alpha=16, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    from transformers import TrainerCallback, TrainerControl\n",
    "\n",
    "    class SampleTextCallback(TrainerCallback):\n",
    "        def __init__(\n",
    "            self, model, tokenizer, output_dir, prompt_text=\"This\", max_length=64\n",
    "        ):\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "            self.output_dir = output_dir\n",
    "            self.prompt_text = prompt_text\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def on_step_begin(self, args, state, control, **kwargs):\n",
    "            import os\n",
    "\n",
    "            if state.global_step % 50 == 0 and state.global_step > 0:\n",
    "                input_ids = self.tokenizer.encode(\n",
    "                    self.prompt_text, return_tensors=\"pt\"\n",
    "                ).to(self.model.device)\n",
    "                sample_outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_length=self.max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=1.0,\n",
    "                )\n",
    "                text = f\"\\n{state.global_step}: {self.tokenizer.decode(sample_outputs[0], skip_special_tokens=True)}\"\n",
    "                print(text)\n",
    "\n",
    "                sample_file_path = os.path.join(\n",
    "                    self.output_dir, f\"training_samples.txt\"\n",
    "                )\n",
    "                with open(sample_file_path, \"a\") as file:\n",
    "                    file.write(text)\n",
    "\n",
    "    trainer_callbacks = [\n",
    "        SampleTextCallback(\n",
    "            model_manager.model,\n",
    "            tokenization_manager.tokenizer,\n",
    "            \"/scratch/vgn2004/fine_tuning/standard\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_manager.model,\n",
    "        train_dataset=training_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        callbacks=trainer_callbacks,\n",
    "        args=TrainingArguments(\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            num_train_epochs=50,\n",
    "            learning_rate=2e-4,\n",
    "            logging_strategy=IntervalStrategy.STEPS,\n",
    "            logging_steps=25,\n",
    "            evaluation_strategy=IntervalStrategy.STEPS,\n",
    "            eval_steps=500,\n",
    "            save_strategy=IntervalStrategy.STEPS,\n",
    "            save_steps=1000,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            output_dir=\"/scratch/vgn2004/fine_tuning/standard\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenization_manager.tokenizer,\n",
    "            mlm=False,  # For causal LM; set to True if you're using a masked LM like BERT\n",
    "        ),\n",
    "    )\n",
    "    model_manager.model.config.use_cache = (\n",
    "        False  # silence the warnings. Please re-enable for inference!\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
