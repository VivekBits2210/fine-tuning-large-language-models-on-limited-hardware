{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/3.8.6/intel/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import gc\n",
    "import bitsandbytes\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from managers import SystemMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "    \"TRANSFORMERS_NO_ADVISORY_WARNINGS\": \"true\",\n",
    "    \"TORCHDYNAMO_DISABLE\": \"1\",\n",
    "    \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "}\n",
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "random.seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baseline usage: 0 GB of GPU'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = SystemMonitor()\n",
    "f\"Baseline usage: {monitor.get_gpu_utilization()} GB of GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurations\n",
    "class Configuration:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.keep_fraction = kwargs.get(\"keep_fraction\", 0.99)\n",
    "        self.test_fraction = kwargs.get(\"test_fraction\", 0.2)\n",
    "        self.scratch_path = kwargs.get(\"scratch_path\", \"/scratch/vgn2004\")\n",
    "        self.dataset_path = kwargs.get(\"dataset_path\", os.path.join(\n",
    "            self.scratch_path, \"fine_tuning\", \"datasets\", \"disaster_tweets.csv\"\n",
    "        ))\n",
    "        self.num_workers = kwargs.get(\"num_workers\", 14)\n",
    "        self.num_virtual_tokens = kwargs.get(\"num_virtual_tokens\", 16)\n",
    "        self.batch_size = kwargs.get(\"batch_size\", 128)\n",
    "        self.lr = kwargs.get(\"lr\", 3e-4)\n",
    "        self.num_epochs = kwargs.get(\"num_epochs\", 5)\n",
    "        self.max_length = kwargs.get(\"max_length\", 128)\n",
    "        self.device = kwargs.get(\"device\", \"cuda\")\n",
    "        \n",
    "        self.model_name_or_path = kwargs.get(\"model_name_or_path\", \"NousResearch/Llama-2-7b-hf\")\n",
    "        \n",
    "        self.r = kwargs.get(\"r\", 64)\n",
    "        self.lora_alpha = kwargs.get(\"lora_alpha\", 128)\n",
    "        self.lora_dropout = kwargs.get(\"lora_dropout\", 0.2)\n",
    "        self.lora_bias = kwargs.get(\"lora_bias\", \"none\")\n",
    "        self.is_gradient_checkpointing_enabled = kwargs.get(\"is_gradient_checkpointing_enabled\", True)\n",
    "        \n",
    "        self.is_quantized = kwargs.get(\"is_quantized\", False)\n",
    "\n",
    "config = Configuration() #model_name_or_path=\"facebook/opt-1.3b\")\n",
    "config.keep_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, prepare_model_for_kbit_training, PromptTuningInit, PromptTuningConfig, LoraConfig, TaskType\n",
    "if 'LLama' in config.model_name_or_path:\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(config.model_name_or_path)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    tokenizer.model_max_length = config.max_length\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63da5ed8b6343ebb7c4bbca7f6a9405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,570,816 || all params: 6,640,914,432 || trainable%: 0.5055149609854211\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "if config.is_quantized:\n",
    "    if 'LLama' in config.model_name_or_path:\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            config.model_name_or_path,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.model_name_or_path,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "else:\n",
    "    if 'LLama' in config.model_name_or_path:\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            config.model_name_or_path\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.model_name_or_path\n",
    "        )\n",
    "    \n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if config.is_gradient_checkpointing_enabled:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    target_modules = find_all_linear_names(model),\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    inference_mode=False, \n",
    "    r=config.r, \n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"NousResearch/Llama-2-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 9941,\n",
       " 'keyword': 'trouble',\n",
       " 'location': None,\n",
       " 'text': \"The worst  voice I can ever hear is the 'Nikki your in trouble' voice from my mom\",\n",
       " 'target': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files=config.dataset_path)\n",
    "dataset = dataset['train'].train_test_split(test_size=config.test_fraction)\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0908c6f24c4d50b5061b794567d0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=14):   0%|          | 0/6090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5809f263bf42b0af2c7fad0668dcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=14):   0%|          | 0/1523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['text'], max_length=config.max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = examples['target']\n",
    "    return model_inputs\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=config.num_workers,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('A sinkhole grows in Brooklyn: six-meter crater swallows street http://t.co/gkPrvzQ6lk',\n",
       " 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = processed_datasets['train'][33]\n",
    "ids = data['input_ids']\n",
    "print(len(ids))\n",
    "tokenizer.decode(ids, skip_special_tokens=True), data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(processed_datasets['train'], sampler=torch.utils.data.RandomSampler(processed_datasets['train']), batch_size=config.batch_size, num_workers=config.num_workers, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\"), pin_memory=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(processed_datasets['test'], sampler=torch.utils.data.SequentialSampler(processed_datasets['test']), batch_size=config.batch_size, num_workers=config.num_workers, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\"), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr) if not config.is_quantized else bitsandbytes.optim.AdamW(model.parameters(), lr=config.lr, is_paged=True, optim_bits=8)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(training_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "def calculate_metrics(preds, labels):\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return precision, recall, accuracy, f1\n",
    "\n",
    "# Evaluate a dataloader\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            batch = {k: v.to(config.device) for k, v in data.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            preds = torch.argmax(torch.softmax(outputs.logits, dim=1), dim=1)\n",
    "            labels = batch['labels']\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    precision, recall, accuracy, f1 = calculate_metrics(all_preds, all_labels)\n",
    "    return precision, recall, accuracy, f1, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: 25 GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/48 [00:35<27:37, 35.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: 38 GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/48 [01:06<25:16, 32.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: 38 GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 3/48 [01:36<23:39, 31.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: 38 GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/48 [02:04<22:10, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: 38 GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 47/48 [26:49<00:35, 35.68s/it]"
     ]
    }
   ],
   "source": [
    "if not config.is_quantized:\n",
    "    model.to(config.device)\n",
    "\n",
    "exit = False\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(training_dataloader)):\n",
    "        if epoch==0 and step < 5:\n",
    "            print(f\"Usage: {monitor.get_gpu_utilization()} GB of GPU\")\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(config.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss detected at Epoch {epoch}, Step {step}\")\n",
    "            exit = True\n",
    "            break\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    if exit:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    precision_val, recall_val, accuracy_val, f1_val, eval_loss = evaluate(validation_dataloader)\n",
    "    print(f\"Validation Data - Precision: {precision_val}, Recall: {recall_val}, Accuracy: {accuracy_val}, F1: {f1_val}\")\n",
    "    eval_epoch_loss = eval_loss / len(validation_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(training_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiments 128 sequence length\n",
    "#LLama 7b - 83% accuracy, 42 GB on GPU, 6? minutes per epoch, 2 epochs\n",
    "#OPT 1.3b - 68% accuracy, 12 GB on GPU, 2.5 minutes per epoch, 2 epochs\n",
    "#OPT 1.3b - 77% accuracy, 12 GB on GPU, 2.5 minutes per epoch, 10 epochs, 16 batch size,  3e-3\n",
    "#OPT 1.3b - 72% accuracy, 26 GB on GPU, 3 minutes per epoch, 2 epochs, 64 batch size, 3e-3\n",
    "#OPT 1.3b - 83.8% accuracy, 26 GB on GPU, 3 minutes per epoch, 10 epochs, 64 batch size, 3e-4,\n",
    "#OPT 1.3b - 79.9% accuracy,  21 GB on GPU, 3 minutes per epoch, 2 epochs, 64 batch size, 3e-4, BIGGER r + alpha\n",
    "#OPT 1.3b - 79.18% accuracy, 21 GB on GPU, 3 minutes per epoch, 2 epochs, 64 batch size, 6e-4, BIGGER r + alpha\n",
    "#OPT 1.3b - 83% accuracy, 12 GB on GPU, 3 minutes per epoch, 5 epochs, 32 batch size, 3e-4, r=64, alpha = 128, dropout=0.2\n",
    "#OPT 1.3b - 83% accuracy, 6 GB on GPU, 4.25 minutes per epoch, 5 epochs, 32 batch size, 3e-4, r=64, alpha = 128, dropout=0.2, grad checkpointing\n",
    "#OPT 1.3b - 83.9% accuracy, 2 GB on GPU, 6.5 minutes per epoch, 5 epochs, 32 batch size, 3e-4, r=64, alpha = 128, dropout=0.2, grad checkpointing, quantized-4-bit\n",
    "#OPT 1.3b - 83.9% accuracy, 4 GB on GPU, 7 minutes per epoch, 5 epochs, 128 batch size, 3e-4, r=64, alpha = 128, dropout=0.2, grad checkpointing, quantized-4-bit\n",
    "\n",
    "#Llama 7B with best settings from above -  accuracy, 6 GB on GPU, 40 minutes per epoch\n",
    "#Llama 7B with best settings from above -  accuracy,  GB on GPU, 40 minutes per epoch\n",
    "\n",
    "#Llama 30B with best settings from above -  accuracy,  GB on GPU,   minutes per epoch\n",
    "\n",
    "# Implement Llama - \n",
    "# NousResearch/Llama-2-13b-hf\n",
    "# NousResearch/Llama-2-70b-hf\n",
    "# Prompting inference on instruct-tuned llamma\n",
    " \n",
    "\n",
    "#OPT 1.3b -  accuracy,  GB on GPU,   minutes per epoch, base prompting\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, data in enumerate(training_dataloader):\n",
    "#         batch = {k: v.to(config.device) for k, v in data.items()}\n",
    "#         print([(t[0].item(),t[1].item()) for t  in list(zip(torch.argmax(torch.softmax(model(**batch).logits, dim=1),dim=1), batch['labels']))])\n",
    "#         if i>20:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating metrics for training data\n",
    "# precision_train, recall_train, accuracy_train, f1_train = evaluate(training_dataloader)\n",
    "# print(f\"Training Data - Precision: {precision_train}, Recall: {recall_train}, Accuracy: {accuracy_train}, F1: {f1_train}\")\n",
    "\n",
    "# Calculating metrics for validation data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
