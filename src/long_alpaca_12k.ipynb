{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import bitsandbytes\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
    "    FullOptimStateDictConfig,\n",
    "    FullStateDictConfig,\n",
    ")\n",
    "from accelerate import Accelerator, FullyShardedDataParallelPlugin\n",
    "from psutil import Process\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetCount,\n",
    ")\n",
    "\n",
    "\n",
    "class SystemMonitor:\n",
    "    def __init__(self):\n",
    "        # Initialize NVML for GPU monitoring\n",
    "        self.nvml_initialized = SystemMonitor._initialize_nvml()\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_nvml(cls):\n",
    "        try:\n",
    "            nvmlInit()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing NVML: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_ram_usage(self):\n",
    "        return Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    def get_gpu_memory_usage(self):\n",
    "        if not self.nvml_initialized:\n",
    "            print(\"NVML not initialized.\")\n",
    "            return None\n",
    "\n",
    "        gpu_memory_usage = []\n",
    "        try:\n",
    "            gpu_count = nvmlDeviceGetCount()\n",
    "            for i in range(gpu_count):\n",
    "                handle = nvmlDeviceGetHandleByIndex(i)\n",
    "                info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                gpu_memory_usage.append(info.used // 1024**3)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving GPU memory info: {e}\")\n",
    "            return None\n",
    "\n",
    "        return gpu_memory_usage\n",
    "\n",
    "    def get_gpu_utilization(self):\n",
    "        gpu_memory_usages = self.get_gpu_memory_usage()\n",
    "        return gpu_memory_usages if gpu_memory_usages is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "\n",
    "class TokenizerHelper:\n",
    "    def __init__(self, prompter, tokenizer, cutoff_len):\n",
    "        self.prompter = prompter\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_on_inputs = True\n",
    "        self.add_eos_token = True\n",
    "        self.cutoff_len = cutoff_len\n",
    "\n",
    "    def tokenize(self, prompt):\n",
    "        result = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(self, data_point):\n",
    "        full_prompt = self.prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        tokenized_full_prompt = self.tokenize(full_prompt)\n",
    "\n",
    "        if not self.train_on_inputs:\n",
    "            user_prompt = self.prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = self.tokenize(user_prompt)\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if self.add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"input_ids\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        else:\n",
    "            tokenized_full_prompt[\"labels\"] = tokenized_full_prompt[\"input_ids\"]\n",
    "        # print(tokenized_full_prompt)\n",
    "        return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "set_seed(1001)\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(\n",
    "        offload_to_cpu=True, rank0_only=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.device_count = torch.cuda.device_count()\n",
    "        self.experiment_name = kwargs.get(\"experiment_name\", \"default_experiment\")\n",
    "        self.keep_fraction = kwargs.get(\"keep_fraction\", 0.99)\n",
    "        self.test_fraction = kwargs.get(\"test_fraction\", 0.2)\n",
    "        self.scratch_path = kwargs.get(\"scratch_path\", \"/scratch/vgn2004\")\n",
    "        self.num_workers = kwargs.get(\"num_workers\", 8)\n",
    "        self.batch_size = kwargs.get(\"batch_size\", 8)\n",
    "        self.lr = kwargs.get(\"lr\", 3e-4)\n",
    "        self.num_epochs = kwargs.get(\"num_epochs\", 5)\n",
    "        self.seq_length = kwargs.get(\"seq_length\", 32768)\n",
    "        self.device = kwargs.get(\"device\", accelerator.device)\n",
    "        self.device_map = kwargs.get(\"device_map\", \"auto\")\n",
    "        self.max_gpu_memory = kwargs.get(\"max_gpu_memory\", \"45080MB\")\n",
    "        # self.device_map = kwargs.get(\"device_map\", {\"\": accelerator.process_index})\n",
    "\n",
    "        self.model_name_or_path = kwargs.get(\n",
    "            \"model_name_or_path\",\n",
    "            \"togethercomputer/LLaMA-2-7B-32K\",  # \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "        )\n",
    "\n",
    "        self.r = kwargs.get(\"r\", 16)\n",
    "        self.lora_alpha = kwargs.get(\"lora_alpha\", 64)\n",
    "        self.lora_dropout = kwargs.get(\"lora_dropout\", 0.2)\n",
    "        self.lora_bias = kwargs.get(\"lora_bias\", \"none\")\n",
    "        self.is_gradient_checkpointing_enabled = kwargs.get(\n",
    "            \"is_gradient_checkpointing_enabled\", True\n",
    "        )\n",
    "        self.is_gradient_accumulation_enabled = kwargs.get(\n",
    "            \"is_gradient_accumulation_enabled\", True\n",
    "        )\n",
    "        self.gradient_accumulation_steps = kwargs.get(\n",
    "            \"gradient_accumulation_steps\", self.batch_size\n",
    "        )\n",
    "        self.batch_size = 1\n",
    "\n",
    "        self.is_quantized = kwargs.get(\"is_quantized\", True)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(f\"{k}: {v}\" for k, v in vars(self).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWARGS: {'experiment_name': 'default_experiment', 'f': '/home/vgn2004/.local/share/jupyter/runtime/kernel-4e2efc4c-f2bf-471a-b434-16f2153f95b2.json'}\n",
      "Configuration: \n",
      "device_count: 1\n",
      "experiment_name: default_experiment\n",
      "keep_fraction: 0.99\n",
      "test_fraction: 0.2\n",
      "scratch_path: /scratch/vgn2004\n",
      "num_workers: 8\n",
      "batch_size: 1\n",
      "lr: 0.0003\n",
      "num_epochs: 5\n",
      "seq_length: 2024\n",
      "device: cuda\n",
      "device_map: auto\n",
      "max_gpu_memory: 45080MB\n",
      "model_name_or_path: togethercomputer/LLaMA-2-7B-32K\n",
      "r: 16\n",
      "lora_alpha: 64\n",
      "lora_dropout: 0.2\n",
      "lora_bias: none\n",
      "is_gradient_checkpointing_enabled: True\n",
      "is_gradient_accumulation_enabled: True\n",
      "gradient_accumulation_steps: 8\n",
      "is_quantized: True\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Fine-tuning configuration\")\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"default_experiment\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "kwargs = vars(args)\n",
    "kwargs.update(\n",
    "    dict((arg[0].lstrip(\"-\"), arg[1]) for arg in zip(unknown[::2], unknown[1::2]))\n",
    ")\n",
    "print(f\"KWARGS: {kwargs}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# os.environ.update(env_vars)\n",
    "\n",
    "config = Configuration(**kwargs)\n",
    "print(f\"Configuration: \\n{config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline usage: [0] GB of GPU\n"
     ]
    }
   ],
   "source": [
    "monitor = SystemMonitor()\n",
    "print(f\"Baseline usage: {monitor.get_gpu_utilization()} GB of GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', '</s>')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
    "tokenizer.model_max_length = config.seq_length\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"togethercomputer/LLaMA-2-7B-32K\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"auto_map\": {\n",
       "    \"AutoModelForCausalLM\": \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2024,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"type\": \"linear\"\n",
       "  },\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(config.model_name_or_path)\n",
    "model_config.max_position_embeddings = config.seq_length\n",
    "model_config.bos_token_id = tokenizer.bos_token_id\n",
    "model_config.eos_token_id = tokenizer.eos_token_id\n",
    "model_config.pad_token_id = tokenizer.pad_token_id\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58075791a39e43df831d9cfca7ebd0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "if config.is_quantized:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name_or_path,\n",
    "        config=model_config,\n",
    "        device_map=config.device_map,\n",
    "        quantization_config=quantization_config,\n",
    "        max_memory={i: config.max_gpu_memory for i in range(config.device_count)},\n",
    "        trust_remote_code=False,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(config.model_name_or_path)\n",
    "\n",
    "if config.is_gradient_checkpointing_enabled:\n",
    "    model.config.use_cache = False\n",
    "    model.enable_input_require_grads()\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Model settings\n",
    "model.config.pretraining_tp = 1\n",
    "model.config.torch_dtype = torch.float32\n",
    "setattr(model, \"model_parallel\", True)\n",
    "setattr(model, \"is_parallelizable\", True)\n",
    "\n",
    "\n",
    "def find_all_linear_names(m):\n",
    "    cls = bitsandbytes.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in m.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=config.r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model, use_gradient_checkpointing=config.is_gradient_checkpointing_enabled\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"togethercomputer/LLaMA-2-7B-32K\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForCausalLM\": \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2024,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "torch.float32 262410240 0.07496550989769399\n",
      "torch.uint8 3238002688 0.925034490102306\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "\n",
    "# Print dtypes\n",
    "dtypes = {}\n",
    "for _, p in model.named_parameters():\n",
    "    dtype = p.dtype\n",
    "    if dtype not in dtypes:\n",
    "        dtypes[dtype] = 0\n",
    "    dtypes[dtype] += p.numel()\n",
    "total = 0\n",
    "for k, v in dtypes.items():\n",
    "    total += v\n",
    "for k, v in dtypes.items():\n",
    "    print(k, v, v / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  6 00:33:50 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Quadro RTX 8000                On  | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   44C    P0              67W / 250W |   5075MiB / 46080MiB |      4%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A   2760831      C   .../apps/python/3.8.6/intel/bin/python     5072MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  input_context = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# Write a summary of the following article:\n",
    "# With new inventions and the advancement of technology, as well as the increased popularity of having online relations on social networking websites, online communication has become a common occurrence for people all over the world. Due to this sudden advancement, there has been a debate regarding the use of online social networking over face-to-face communications and relationships. Due to people’s ability to express their true self on computer mediated conversations, along with the formation of secure online romantic relationships and positive friendships, this paper will argue that online relations are just as good, if not better, than face-to-face interactions.\n",
    "# To begin, ones true self is whom a person actually is whether they choose to.\n",
    "# This was evident in a study conducted at New York University (Bargh et al., 2002).\n",
    "# In this study, participants were asked to sort a series of self-descriptive adjectives as they related to them; categorizing them as either “me” or “not me”. Following that activity, participants were randomly assigned to a face-to-face condition group or an online condition group where they were matched with partners and instructed to begin interacting. This study found that participants in the Internet condition group were faster to respond to their actual self traits, whereas those in the in-person interaction conditions were not able to sort the aforementioned adjectives at the same rate. This supports the claim that an online interaction leads to the activation of ones true self qualities. Therefore, the Internet gives people the confidence and means to express their true self and behave in positive ways that they normally would not if placed in a face-to-face interaction.\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "# # Encode the input context\n",
    "# input_ids = tokenizer.encode(input_context, return_tensors=\"pt\", padding=False, truncation=True).to(\"cuda\")\n",
    "# with torch.inference_mode():\n",
    "#     output = model.generate(input_ids=input_ids, max_length=512, eos_token_id=29937, top_k=50, temperature=1.0)\n",
    "\n",
    "#     # Decode the output\n",
    "#     output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Print the result\n",
    "#     print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Yukang/LongAlpaca-12k\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Below is a paper. Memorize the paper and answer my question after the paper.\n",
      " The paper begins. \n",
      " Abstract\n",
      "be used either figuratively or literally, in a context dependent manner: For example, the phrase \"clean can be interpreted literally, as in We \n",
      "\n",
      "### Response:\n",
      "The work opens up many possibilities for future research including:\n",
      "\n",
      "1. Extending the proposed framework to more datasets and tasks involving non-compositional language processing. The current work focuses on idiom usage recognition and metaphor detection but the proposed CLCL framework could potentially be applied to other tasks like concept metaphor detection, simile detection, sarcasm detection, etc. \n",
      "\n",
      "2. Further analyzing the differences between non-compositionality in idioms versus metaphors. The results showed some key differences in how the proposed framework performed on idiom usage recognition \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"sft_trainer_output\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     gradient_checkpointing=True,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     logging_steps=10,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-4,\n",
    "#     max_grad_norm=0.3,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "# #     disable_tqdm=False,  # disable tqdm since with packing values are in correct\n",
    "# )\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "def format_instruction(sample) -> str:\n",
    "    # returns the full prompt from instruction and optional input\n",
    "    # if a label (=response, =output) is provided, it's also appended.\n",
    "    template = {\n",
    "        \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "        \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "        \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "    }\n",
    "\n",
    "    if sample[\"input\"]:\n",
    "        res = template[\"prompt_input\"].format(\n",
    "            instruction=sample[\"instruction\"][:250], input=sample[\"input\"][:250]\n",
    "        )\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(\n",
    "            instruction=sample[\"instruction\"][:250]\n",
    "        )\n",
    "    if sample[\"output\"]:\n",
    "        res = f\"{res}{sample['output']}\"\n",
    "    return res\n",
    "\n",
    "\n",
    "print(format_instruction(dataset[\"train\"][2])[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(element):\n",
    "    return tokenizer(\n",
    "        format_instruction(element),\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=config.seq_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fe953dca2540058d0b873d58176c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=8):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3f9dec9181422782bd2af1a5cdb40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=8):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = (\n",
    "    dataset[\"train\"]\n",
    "    .select(range(20))\n",
    "    .map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        num_proc=config.num_workers,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenized_test = (\n",
    "    dataset[\"test\"]\n",
    "    .select(range(20))\n",
    "    .map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        num_proc=config.num_workers,\n",
    "        remove_columns=dataset[\"test\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template_with_context = (\n",
    "    \"\\n### Response:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    ")\n",
    "response_template_ids = tokenizer.encode(\n",
    "    response_template_with_context, add_special_tokens=False\n",
    ")[\n",
    "    2:\n",
    "]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template_ids, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_test,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['Below is some paragraphs in the book, Don Quixote. Memorize the content and answer my question after the book.\\n afflicted damsels or to the prayers of wise, magisterial, ancient\\nenchanters and sages. In short, Sancho, either you must be whipped by\\nyourself, or they must whip you, or you shan’t be governor.”\\n\\n“Señor,” said Sancho, “won’t two days’ grace be given me in which to\\nconsider what is best for me?”\\n\\n“No, certainly not,” said Merlin; “here, this minute, and on the spot,\\nthe matter must be settled; either Dulcinea will return to the cave of\\nMontesinos and to her former condition of peasant wench, or else in her\\npresent form shall be carried to the Elysian fields, where she will\\nremain waiting until the number of stripes is completed.”\\n\\n“Now then, Sancho!” said the duchess, “show courage, and gratitude for\\nyour master Don Quixote’s bread that you have eaten; we are all bound\\nto oblige and please him for his benevolent disposition and lofty\\nchivalry. Consent to this whipping, my son; to the devil with the\\ndevil, and leave fear to milksops, for ‘a stout heart breaks bad luck,’\\nas you very well know.”\\n\\nTo this Sancho replied with an irrelevant remark, which, addressing\\nMerlin, he made to him, “Will your worship tell me, Señor Merlin—when\\nthat courier devil came up he gave my master a message from Señor\\nMontesinos, charging him to wait for him here, as he was coming to\\narrange how the lady Doña Dulcinea del Toboso was to be disenchanted;\\nbut up to the present we have not seen Montesinos, nor anything like\\nhim.”\\n\\nTo which Merlin made answer, “The devil, Sancho, is a blockhead and a\\ngreat scoundrel; I sent him to look for your master, but not with a\\nmessage from Montesinos but from myself; for Montesinos is in his cave\\nexpecting, or more properly speaking, waiting for his disenchantment;\\nfor there’s the tail to be skinned yet for him; if he owes you\\nanything, or you have any business to transact with him, I’ll bring him\\nto you and put him where you choose; but for the present make up your\\nmind to consent to this penance, and believe me it will be very good\\nfor you, for soul as well for body—for your soul because of the charity\\nwith which you perform it, for your body because I know that you are of\\na sanguine habit and it will do you no harm to draw a little blood.”\\n\\n“There are a great many doctors in the world; even the enchanters are\\ndoctors,” said Sancho; “however, as everybody tells me the same\\nthing—though I can’t see it myself—I say I am willing to give myself\\nthe three thousand three hundred lashes, provided I am to lay them on\\nwhenever I like, without any fixing of days or times; and I’ll try and\\nget out of debt as quickly as I can, that the world may enjoy the\\nbeauty of the lady Dulcinea del Toboso; as it seems, contrary to what I\\nthought, that she is beautiful after all. It must be a condition, too,\\nthat I am not to be bound to draw blood with the scourge, and that if\\nany of the lashes happen to be fly-flappers they are to count. Item,\\nthat, in case I should make any mistake in the reckoning, Señor Merlin,\\nas he knows everything, is to keep count, and let me know how many are\\nstill wanting or over the number.”\\n\\n“There will be no need to let you know of any over,” said Merlin,\\n“because, when you reach the full number, the lady Dulcinea will at\\nonce, and that very instant, be disenchanted, and will come in her\\ngratitude to seek out the worthy Sancho, and thank him, and even reward\\nhim for the good work. So you have no cause to be uneasy about stripes\\ntoo many or too few; heaven forbid I should cheat anyone of even a hair\\nof his head.”\\n\\n“Well then, in God’s hands be it,” said Sancho; “in the hard case I’m\\nin I give in; I say I accept the penance on the conditions laid down.”\\n\\nThe instant Sancho uttered these last words the music of the clarions\\nstruck up once more, and again a host of muskets were discharged, and\\nDon Quixote hung on Sancho’s neck kissing him again and again on the\\nforehead and cheeks. The duchess and the duke expressed the greatest\\nsatisfaction, the car began to move on, and as it passed the fair\\nDulcinea bowed to the duke and duchess and made a low curtsey to\\nSancho.\\n\\n\\n\\np35c.jpg (284K)\\n\\nFull Size\\n\\n\\n\\nAnd now bright smiling dawn came on apace; the flowers of the field,\\nrevived, raised up their heads, and the crystal waters of the brooks,\\nmurmuring over the grey and white pebbles, hastened to pay their\\ntribute to the expectant rivers; the glad earth, the unclouded sky, the\\nfresh breeze, the clear light, each and all showed that the day that\\ncame treading on the skirts of morning would be calm and bright. The\\nduke and duchess, pleased with their hunt and at having carried out\\ntheir plans so cleverly and successfully, returned to their castle\\nresolved to follow up their joke; for to them there was no reality that\\ncould afford them more amusement.\\n\\n\\n\\np35e.jpg (10K)\\n\\n\\n\\nCHAPTER XXXVI.\\nWHEREIN IS RELATED THE STRANGE AND UNDREAMT-OF ADVENTURE OF THE\\nDISTRESSED DUENNA, ALIAS THE COUNTESS TRIFALDI, TOGETHER WITH A LETTER\\nWHICH SANCHO PANZA WROTE TO HIS WIFE, TERESA PANZA\\n\\n\\n\\n\\np36a.jpg (150K)\\n\\nFull Size\\n\\n\\n\\nThe duke had a majordomo of a very facetious and sportive turn, and he\\nit was that played the part of Merlin, made all the arrangements for\\nthe late adventure, composed the verses, and got a page to represent\\nDulcinea; and now, with the assistance of his master and mistress, he\\ngot up another of the drollest and strangest contrivances that can be\\nimagined.\\n\\nThe duchess asked Sancho the next day if he had made a beginning with\\nhis penance task which he had to perform for the disenchantment of\\nDulcinea. He said he had, and had given himself five lashes overnight.\\n\\nThe duchess asked him what he had given them with.\\n\\nHe said with his hand.\\n\\n“That,” said the duchess, “is more like giving oneself slaps than\\nlashes; I am sure the sage Merlin will not be satisfied with such\\ntenderness; worthy Sancho must make a scourge with claws, or a\\ncat-o’-nine tails, that will make itself felt; for it’s with blood that\\nletters enter, and the release of so great a lady as Dulcinea will not\\nbe granted so cheaply, or at such a paltry price; and remember, Sancho,\\nthat works of charity done in a lukewarm and half-hearted way are\\nwithout merit and of no avail.”\\n\\nTo which Sancho replied, “If your ladyship will give me a proper\\nscourge or cord, I’ll lay on with it, provided it does not hurt too\\nmuch; for you must know, bo This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['Below is a paper. Memorize the paper and answer my question after the paper.\\n The paper begins. \\n Abstract\\nwe show that by simply defining a cross-attention based correspondence between the input text tokens and the user stroke-painting, the user is also able to control the seman- tics of different painted without requiring any con- regions ditional training or finetuning: Human user study results show that the proposed approach outperforms the previous state-of-the-art by over 85.329 on the overall user satis- Project page for our paper is available at faction scores: /1jsingh. github. io/gradop https\\nControllable image  synthesis user  scribbles has with gained public interest with the recent advent of text- huge conditioned latent diffusion models: The user scribbles con- trol the color composition while the text prompt provides control over the overall image semantics: However; we note works in this direction suffer from an intrinsic that prior domain shift problem wherein the generated outputs often lack details and resemble simplistic representations of the target domain In this paper; we propose a novel guided im- which addresses this problem by age synthesis framework; modelling the output image aS the solution of a constrained optimization problem. We show that while computing an exact solution to the optimization is infeasible, an approx- imation of the same can be achieved while just requiring single pass of the reverse diffusion process: Additionally;\\n1. Introduction\\nGuided image synthesis with user scribbles has gained widespread public attention with the recent advent of large- scale language-image (LLI) models [23,26,28,30,40]. A\\nnovice user can significant control over the final image gain contents by combining text-based conditioning with unsu- pervised guidance from a reference image (usually a coarse stroke painting). The text prompt controls the overall image semantics, while the provided coarse stroke painting allows the user to define the color composition in the output scene_\\ngions can be inferred as blue-green grass instead of a river: we show that by simply defining a cross- this, To address attention based correspondence between the input text to- kens and user stroke-painting; the user can control seman- tics of different painted regions without requiring semantic- segmentation based conditional training O finetuning:\\nExisting   methods often aim   attempt achieve this to The first category leverages condi- through two means. training using semantic segmentation maps [8, 28, tional 39]. However; the conditional training itself is quite time- consuming and requires a large scale collection of dense se- mantic segmentation labels across diverse data modalities The second category, typically leverages an inversion based approach for mapping the input stroke painting to the tar- get data manifold without requiring any paired annotations popular solution by [22,35] introduces the For instance, by considering painting based generative noisy ver- prior sion of the original image as the start of the reverse diffusion process.  However; the use of an inversion based approach causes an intrinsic domain shift problem if the domain gap between the provided stroke painting and the target domain is too high. In particular; we observe that the resulting out- puts often lack details and resemble simplistic representa- tions of the target domain. For instance, in Fig: we notice that while the target domain consists of realistic photos of a landscape, the generated outputs resemble simple pictorial Iteratively reperforming arts which are not very realistic. the guided synthesis with the generated outputs [4] seems blurry improve realism but it is costly, some details still to persist (refer Fig: 4), and the generated outputs tend to lose faithfulness to the reference with each successive iteration_\\n2. Related Work\\nGAN-based  methods have been extensively explored image synthesis   from for performing guided coarse user scribbles. [1-3,14,27,37,42] use GAN-inversion for prO- jecting user scribbles on to manifold of real images. While for performing small scale inferences these methods good fail to generate highly photorealistic outputs when the given stroke image is too far from the real image manifold. Con- ditional GANs [7,13,18,21,24,36,43] learn to directly gen- erate realistic outputs based on user-editable semantic seg- In another work, Singh et al. [34] propose mentation maps_ image synthesis framework which leverages autonomous an 19, 32,33,44] for inferring photorealistic painting agents outputs from rudimentary user scribbles_ Despite its effi- cacy; this requires the creation of a new dataset and condi- training for each target domain, which is expensive tional\\nGuided image synthesis with LLI models [6,23,26,28, 30,40,41] has widespread attention [9,12,15,17,20, gained 29, 31] due to their ability to perform high quality image generation from diverse target modalities. Of particular in- guidance is provided using terest are works wherein the coarse stroke painting and the model learns to generate out- puts conditioned on both text and painting: Current works in this direction typically 1) use semantic segmentation based conditional training [8, 28, 39] which is expensive, O1;, 2) adopt an inversion-based approach for mapping the input stroke painting to the target data manifold without requiring Meng paired annotations For instance, et al. [22] propose image synthesis framework; wherein the generative kguided noisy version is introduced by simply considering prior of the original sketch input as the start of the reverse dif- fusion process_ Choi et al. [5] propose an iterative con- ditioning strategy wherein the intermediate diffusion out- puts are successively refined t0 move towards the reference image. While effective, use of an inversion-like ap the implicit domain shift problem, proach causes wherein an the output images though faithful to the provided reference Iteratively reperform- show blurry or less textured details. synthesis with generated outputs [4] seems to ing guided improve realism but it is costly. In contrast; we show that it is possible to perform highly photorealistic image synthesis while just requiring a single reverse diffusion pass.\\nthis; To address diffusion-based guided im- we propose age synthesis framework which models the output image as the solution of a constrained optimization problem (Sec. 3). reference painting y, the constrained optimization Given is posed so as to find a solution x with two constraints: 1) upon painting % with an autonomous painting function we painting similar t0 reference y, and, 2) the should recover output € should lie in the target data subspace defined by the text prompt (i.e , if the prompt says then we want \\'photo the output images to be realistic photos instead of cartoon- like representations of the same concept) . Subsequently, we show that while the computation of an exact solution for this optimization is infeasible, a practical approximation of the same can be achieved through simple gradient descent.\\nFinally, while the proposed optimization allows the user generate image outputs with high realism and faithful- to ness (with reference y), the fine-grain semantics of differ- painting regions are inferred implicitly by the diffusion ent Such inference is typically dependent on the model: gener- ative priors learned by the diffusion model, and might not drawing particular accurately reflect the user s intent in region. For instance, in Fig: we see that the light blue re-\\nCross-attention control. Recently, Hertz et al. [10] pro- image editing approach with text- pose a prompt-to-prompt conditioned diffusion models_ By constraining the cross- attention features of all non-targeted text tokens t0 remain\\nFigure Method Overview: a) Given a reference painting y and text prompt Ttezt, we formulate the guided synthesis output as the 2 optimization problem with 2 properties: 1) x lies in the subspace Sttert solution x of a constrained of outputs conditioned only on the text; and; 2) upon painting € we should recover reference painting y. While computing an exact solution of this optimization is infeasible approximation can be obtained in (b). Here we first solve an unconstrained approximation of the original optimization to we show that an being faithful to the reference y: Sttert which is close to a random sample in the latent space, while still This point compute a Tttert using the diffusion based inversion from [35] t0 get the final image output Sttert is then mapped back to the target subspace\\n3.1. GradOP: Obtaining an Appro This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "[\"Below is a paper. Memorize the paper and answer my question after the paper.\\n The paper begins. \\n Abstract\\nMulti-scale features have been proven highly effective huge come with for object detection but  often and even prohibitive extra computation costs, especially for the re - cent Transformer-based detectors: In this paper; pro- we IMFA pose Iterative Multi-scale Feature Aggregation generic paradigm that enables efficient use of multi-scale The core features in Transformer-based object detectors: idea to exploit sparse multi-scale features from just is few crucial locations; and it is achieved with two novel de- rearranges the Transformer encoder First, IMFA signs decoder pipeline so that the encoded features can be iter atively updated based on the detection predictions  Second IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guid ance of prior detection predictions: As a result, the sam- still highly ben- pled multi-scale features yet are   sparse eficial for object detection Extensive experiments   show that the proposed IMFA boosts the performance of multiple Transformer-based object detectors significantly yet with only slight computational overhead\\nThe proposed Iterative Multi-scale Feature Aggregation Figure generic approach for efficient use of multi-scale fea- (IMFA is a tures in Transformer-based object detectors_ It boosts detection accuracy on multiple object detectors at minimal costs of addi- tional computational overhead. Results are obtained with ResNet- 50. Best viewed in color:\\npromising performance: However;  naively incorporating using FPN in these Transformer-based multi-scale features detectors [4,11,20,29,35,55,66,72] often brings enormous and even unfeasible computation costs, primarily due to the pOOr efficiency of the attention mechanism in processing high-resolution  features_ Concretely, to handle feature spatial size of H x W, ConvNet requires map with a a com- putational cost of O(HW), while the complexity of the at tention mechanism in Transformer-based object detectors is 0(H2W2). To mitigate this issue, Deformable DETR [72] and Sparse DETR [43] replace the original global dense SMCA-DETR [11] attention with attention. sparse re layers to be scale-specific stricts most Transformer encoder one encoder layer to integrate multi-scale fea with only tures. However; as the number of tokens increases quadrati- cally wrt feature map Size (typically 2Ox~8Ox compared to single-scale), these methods are still costly in computation and memory consumption, and rely on special operators like\\n1. Introduction\\nDetecting objects of vastly different scales has always major challenge in object detection [28]. been Fortu- nately, strong evidence [11,22, 25,48, 69, 72] shows that object detectors can significantly benefit from multi-scale while  dealing   with  large   scale features For variation: ConvNet-based object detectors like Faster R-CNN [42] and [49], Feature Pyramid Network (FPN) [25] and its FCOS variants [12,18, 19,30,48,69, 70] have become the go-tol components for exploiting multi-scale features_\\nOther than ConvNet-based object detectors, the recently proposed DEtection TRansformer (DETR) [4] has estab- lished fully end-to-end object  detection paradigm  with\\nmarks equal technical contribution marks corresponding author: Page: https:I github com/ZhangGongjie/IMFA _ Project\\nwhere encoded features can be iteratively updated along with refined detection predictions pipeline al- This new lows to leverage intermediate predictions as guidance for robust and efficient multi-scale feature encoding: sampling strategy for multi-scale We propose a sparse features, which first identifies several promising regions under the guidance of prior detections, then searches sev- keypoints within each promising region, and finally eral samples their features at adaptively selected scales We sig demonstrate that such sparse multi-scale features can nificantly benefit object detection. Based on the two contributions above, we propose Iter ative Multi-scale Feature Aggregation (IMFA) a sim- ple and generic paradigm that enables efficient use of detec - multi-scale features in Transformer-based object boosts   detection performance IMFA  consistently tors_ multiple object detectors, yet remains computationally on efficient   This is the pioneering work that investigates a generic approach for exploiting multi-scale features effi- ciently in Transformer-based object detectors_\\ndeformable attention [72] that introduces extra complexity for deployment: To the best of our knowledge, there is yet generic approach that can efficiently exploit multi-scale no features for Transformer-based object detectors_\\nIn this paper; present Iterative Multi-scale Feature we Aggregation (IMFA), concise and effective technique that generic paradigm for efficient use of multi- can serve as scale features in Transformer-based object detectors. The motivation comes from two key observations: (i) the com- putation of high-resolution features is highly redundant as the background usually occupies most of the image space portion of high-resolution features thus only a   small are useful to object detection; (ii) unlike ConvNet; the Trans- require grid-shaped former' s attention mechanism does not which offers the feasibility of aggregating feature maps, specific multi-scale features only from that regions some are likely to contain objects of interest: The two observa- tions motivate us to sparsely sample multi-scale features from just few informative locations and then aggregate image features in an iterative manner: them with encoded\\nConcretely, IMFA consists of two novel designs in the Transformer-based detection pipelines. First, IMFA rear- ranges the encoder-decoder pipeline so that each encoder is immediately connected layer to its corresponding de- This design enables iterative update of en- coder layer: along with refined detection predic- coded image features tions. Second, IMFA sparsely multi-scale features samples from the feature pyramid generated by the backbone, with the sampling process guided by previous detection predic- redundancy Specifically; motivated by the tions spatial of high-resolution features, IMFA  oly focuses few on promising regions with high likelihood of object occurrence predictions_ Furthermore, inspired by the based on prior significance of objects' keypoints for recognition and lo- calization [39, 59,66, 71], IMFA first searches several key points within each promising region, and then samples use- ful features around these keypoints at adaptively selected scales_ The sampled features are finally fed to the subse quent encoder layer along with the image features encoded layer: by the previous With the two new designs, the prO- posed IMFA aggregates only the most crucial multi-scale features from those informative locations. Since the number aggregated features is small, of the IMFA introduces min- imal computational overhead while consistently improving the detection performance of Transformer-based object de- It is noteworthy that IMFA is a generic paradigm for tectors_ Fig: efficient use of multi-scale features: (i) as shown in it can be easily integrated with multiple Transformer-based object detectors with consistent performance boosts; (ii) as discussed in Section 5.4, IMFA has the potential to boost DETR-like models on tasks beyond object detection:\\n2. Related Work\\nObject  Detection: modern   object  detectors, Most like FCOS [49], Faster R-CNN [42], YOLO [40], and are They have achieved promising results on ConvNet-based_ various detection benchmarks [2, 7, 17, 24,38, 44,48, 54_ However;  these methods   detect  objects by 57, 61, 62]. defining surrogate regression and classification tasks, which rely on many hand-crafted components, such as anchors_ rule-based training target assignment; and non-maximum suppression (NMS) Thus the detection pipelines of these complex, hyper-parameter- ConvNet-based detectors are intensive, and not fully end-to-end, leading to sub-optimal performance Unlike ConvNet-based detectors, the recently proposed DETR [4] has revolutionized the paradigm for ob- using ject detection a Transformer [50] encoder-decoder ar chitecture, eliminating the need for those hand-crafted com- In This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['There are two papers. Memorize them and answer my question after the paper.\\n The first paper begins. \\n Abstract\\nensembling We present LLM-BLEND ER, an framework designed to attain consistently su- perior performance by leveraging the diverse strengths of multiple open-source large lan- guage models (LLMs) Our framework con- sists of two modules: PAIRRANKER and GEN FUSER, addressing the observation that opti mal LLMs for different examples can signif- icantly vary: PAIRRANKER employs spe cialized pairwise comparison method to dis tinguish subtle differences between candidate It jointly encodes the input text and outputs using of candidates, cross-attention pair en coders to determine the superior one. Our re sults demonstrate that PAIRRANKER exhibits the highest correlation with ChatGPT-based ranking: Then GENFUSER aims t0 merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large scale evaluation, we introduce benchmark dataset, MixInstruct, which is mixture featuring oracle of multiple instruction datasets pairwise comparisons. Our LLM-BLEND E R significantly outperform individual LLMs and baseline methods across various metrics, estab- lishing a substantial performance gap.\\nFigure I: Motivation 0f ensembling LLMs. Based on this pie chart about the percentage of examples where optimal LLMs for each LLM ranks Ist, we can see that different examples can significantly vary:\\nBiderman et al,,2023), LLaMA Pythia Touvron 2023), and Flan-TS (Chung et al,, 2022) of- et al., fer a chance t0 fine-tune these models on custom instruction datasets, enabling the development of smaller yet efficient LLMs, such Alpaca,  Vi- as Chiang et al, 2023), OpenAssistant (LAION- cuna AI, 2023) , and MPT (MosaicML, 2023)_\\nIntroduction\\nLarge language models (LLMs) have shown im pressive performance in diverse tasks, primarily due to their capacity to follow instructions and ac- high-quality data, showing a promis- cess extensive, ing future for artificial general intelligence (Bubeck et al,, 2023). However; prominent LLMs such as Chowdhery et al,, 2022) GPT-4 and PaLM are closed-source, restricting insights into their archi- tectures and training data. Open-source LLMs like\\nThe open-source LLMs exhibit diverse strengths and weaknesses due to variations in data, archi- tectures, and hyperparameters, making them com plementary to each other: Figure 1 illustrates the distribution of best LLMs on 5,000 instructions that More ranking details can be found we collected. in Sec. 5.1. Although Vicuna achieves the highest percentage, it ranks first in only 21.229 of the ex amples  Furthermore, the pie chart suggests that the optimal LLMs for different examples can sig- nificantly vary and there is no open-source LLM\\n~The experiments on summarization, translation, and con- strained generation tasks in the version have been moved prior appendix. Instead, to the we mainly present our work in the context of instruction-following data and LLMs in this version.\\nthat dominates the competition. Therefore, it is important to dynamically ensemble these LLMs to generate consistently better responses for each input: Considering the diverse strengths and weak- develop an ensem nesses of LLMs, it is crucial to bling method that harnesses their complementary potentials, leading to improved robustness, gener- alization, and accuracy. By combining their unique contributions, we can alleviate biases, errors, and uncertainties in individual LLMs, resulting in out- puts better aligned with human preferences.\\nSubsequently, we can employ the top-ranked can didate from PAIRRANKER for each input as the Hence, this approach does not rely final result single model for all examples; instead, PAIR - on a RANKER selects the best model for each example by comprehensively comparing all candidate pairs.\\nNonetheless, this approach may constrain the potential to generate even better outputs than the existing candidates  To investigate this possibility, we introduce the GENFUSER (Sec. 4) module to fuse the top K of the N ranked candidates and gen erate an improved output for end-users_ Our goal is to capitalize on the strengths of the top K selected candidates while mitigating their weaknesses.\\nWe introduce LLM-BLENDER, ensem- an bling framework designed to achieve consistently superior performance by mixing the outputs of comprises two multiple LLMs. LLM-BLENDER modules: PAIRRANKER and GENFUSER. Ini- tially, PAIRRANKER compares the outputs from N LLMs, which GENFUSER then fuses to gener- ate the final output from the top K ranked outputs.\\nTo assess the effectiveness of LLM ensembling methods, we introduce a benchmark dataset called MixInstruct (Sec. 2.2). In this dataset; we N=I1 popular open-source LLMs to generate use N candidates for each input across various exist- instruction-following tasks formatted as self- ing instruct (Wang et al,, 2022). The dataset comprises 1OOk training examples and Sk validation examples for training a candidate ranking module like our PAIRRANKER, and Sk test examples with oracle comparisons for automatic evaluation.\\nExisting approaches (Ravaut et al,, 2022a; Liu and Liu, 2021), including the reward model within InstructGPT (Ouyang et al, 2022), for ranking out- {91, from language models (LMs) on pputs YN given input z have mostly focused on individually scoring each yi based on €, employing encoding Although modules in the form of $i fe(x, yi ) . this list-wise ranking objective can be powerful and efficient when candidate differences are appar ensembling it may not be as effective when ent, LLMs. Among the output candidates from LLMs, they candidate differences can be quite subtle, as produced by very sophisticated models and are all may only be marginally better than another: one Even for humans, it can be challenging to gauge candidate quality without direct comparison.\\nresults empirical In Section 5 our on the MixInstruct benchmark   reveal that the framework significantly boosts LLM-BLEND E R performance by ensembling LLMs The overall selections made by PAIRRANKER Outperform any fixed individual LLM models, as indicated by SU - performance in both reference-based met- perior rics and GPT-Rank: By leveraging the top selec- tions from PAIRRANKER, GENFUSER further en- hances response quality through effective fusion into the final output: LLM-BLENDE R achieves the highest scores in terms of both conventional metrics (i.e,, BERTScore, BARTScore, BLUERT) and ChatGPT-based ranking: The average rank stands at 3.2 among the 12 of LLM-BLEND E R methods, which is considerably better than the best LLM s rank of 3.90. Moreover; LLM-BLEND ER \\'s output ranks in the top 3 for 68.59% of examples, while Viccuna only reaches 52.88%. We believe findings would benefit LLM-BLEND E R and our both practitioners and researchers for deploying and studying LLMs with ensemble learning:\\nspecialized pairwise As a result, propose a we comparison method, PAIRRANKER (Sec. 3), to effectively discern subtle differences between can- didate outputs and enhance ranking performance particular; gather the outputs from N In we first Fig; (e.g , 1) for each models the N 11 models in input and subsequently create the N( N 1)/2 pairs of their outputs. We jointly encode the input x and the two candidate outputs %yi and yj as input RoBERTa (Liu (e.g , to a cross-attention encoder et al,, 2019)), in the form of f&(€,Yi, 9;), to learn and determine which candidate is better:\\nDuring the inference stage, we compute a ma- containing logits representing pairwise com trix par This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['Below is some paragraphs in the book, Dream of the Red Chamber. Memorize the content and answer my question after the book.\\n \\n“This is, indeed, strange!” exclaimed Pao-yue. “If you won’t go, what’s the good of all this fuss? I can’t stand this bawling, so it will be a riddance if you would get out of the way!”\\n\\nSaying this, he was resolved upon going to report the matter. Hsi Jen found herself powerless to dissuade him. She had in consequence no other resource but to fall on her knees.\\n\\nPi Hen, Ch’iu Wen, She Yueeh and the rest of the waiting-maids had realised what a serious aspect the dispute had assumed, and not a sound was to be heard to fall from their lips. They remained standing outside listening to what was going on.\\n\\nWhen they now overheard Hsi Jen making solicitous entreaties on her knees, they rushed into the apartment in a body; and with one consent they prostrated themselves on the floor.\\n\\nPao-yue at once pulled Hsi Jen up. Then with a sigh, he took a seat on the bed.\\n\\n“Get up,” he shouted to the body of girls, “and clear out! What would you have me do?” he asked, addressing himself to Hsi Jen. “This heart of mine has been rent to pieces, and no one has any idea about it!”\\n\\nWhile speaking, tears of a sudden rolled down his cheek. At the sight of Pao-yue weeping, Hsi Jen also melted into a fit of crying. Ch’ing Wen was standing by\\n\\nthem, with watery eyes. She was on the point of reasoning with them, when espying Lin Tai-yue step into the room, she speedily walked out.\\n\\n“On a grand holiday like this,” remonstrated Lin Tai-yue smiling, “how is it that you’re snivelling away, and all for nothing? Is it likely that high words have resulted all through that ‘dumpling’ contest?”\\n\\nPao-yue and Lin Tai-yue blurted out laughing.\\n\\n“You don’t tell me, cousin Secundus,” Lin Tai-yue put in, “but I know all about it, even though I have asked no questions.”\\n\\nNow she spoke, and now she patted Hsi Jen on the shoulder. “My dear sister-in-law,” she smiled, “just you tell me! It must surely be that you two have had a quarrel.\\n\\nConfide in me, your cousin, so that I might reconcile you.”\\n\\n“Miss Lin,” rejoined Hsi Jen, pushing her off, “what are you fussing about? I am simply one of our servant-girls; you’re therefore rather erratic in your talk!”\\n\\n“You say that you’re only a servant-girl,” smilingly replied Tai-yue, “and yet I treat you like a sister-in-law.”\\n\\n“Why do you,” Pao-yue chimed in, “give her this abusive epithet? But however much she may make allowance for this, can she, when there are so many others who tell idle tales on her account, put up with your coming and telling her all you’ve said?”\\n\\n“Miss Lin,” smiled Hsi Jen, “you’re not aware of the purpose of my heart. Unless my breath fails and I die, I shall continue in his service.”\\n\\n“If you die,” remarked Lin Tai-yue smiling, “what will others do, I wonder? As for me, I shall be the first to die from crying.”\\n\\n“Were you to die,” added Pao-yue laughingly, “I shall become a bonze.”\\n\\n“You’d better be a little more sober-minded!” laughed Hsi Jen. “What’s the good of coming out with all these things?”\\n\\nLin Tai-yue put out two of her fingers, and puckered up her lips. “Up to this,”\\n\\nshe laughed, “he’s become a bonze twice. Henceforward, I’ll try and remember how many times you make up your mind to become a Buddhist priest!”\\n\\nThis reminded Pao-yue that she was referring to a remark he had made on a previous occasion, but smiling to himself, he allowed the matter to drop.\\n\\nAfter a short interval, Lin Tai-yue went away. A servant then came to announce that Mr. Hsueeh wanted to see him, and Pao-yue had to go. The purpose of this visit was in fact to invite him to a banquet, and as he could not very well put forward any excuse to refuse, he had to remain till the end of the feast before he was able to take his leave. The result was that, on his return, in the evening, he was to a great extent under the effect of wine. With bustling step, he wended his way into his own court.\\n\\nHere he perceived that the cool couch with a back to it, had already been placed in the yard, and that there was some one asleep on it. Prompted by the conviction that it must be Hsi Jen, Pao-yue seated himself on the edge of the couch. As he did so, he gave her a push, and inquired whether her sore place was any better. But thereupon he saw the occupant turn herself round, and exclaim: “What do you come again to irritate me for?”\\n\\nPao-yue, at a glance, realised that it was not Hsi Jen, but Ch’ing Wen. Pao-yue then clutched her and compelled her to sit next to him. “Your disposition,” he smiled,\\n\\n“has been more and more spoilt through indulgence. When you let the fan drop this morning, I simply made one or two remarks, and out you came with that long rigmarole. Had you gone for me it wouldn’t have mattered; but you also dragged in Hsi Jen, who only interfered with every good intention of inducing us to make it up again. But, ponder now, ought you to have done it; yes or no?”\\n\\n“With this intense heat,” remonstrated Ch’ing Wen, “why do you pull me and toss me about? Should any people see you, what will they think? But this person of mine isn’t meet to be seated in here.”\\n\\n“Since you yourself know that it isn’t meet,” replied Pao-yue with a smile, “why then were you sleeping here?”\\n\\nTo this taunt Ch’ing Wen had nothing to say. But she spurted out into fresh laughter. “It was all right,” she retorted, “during your absence; but the moment you come, it isn’t meet for me to stay! Get up and let me go and have my bath. Hsi Jen and She Yueeh have both had theirs, so I’ll call them here!”\\n\\n“I’ve just had again a good deal of wine,” remarked Pao-yue, laughingly; “so a wash will be good for me. And since you’ve not had your bath, you had better bring the water and let’s both have it together.”\\n\\n“No, no!” smiled Ch’ing Wen, waving her hand, “I cannot presume to put you to any trouble, Sir. I still remember how when Pi Hen used to look after your bath you occupied fully two or three hours. What you were up to during that time we\\n\\nnever knew. We could not very well walk in. When you had however done washing, and we entered your room, we found the floor so covered with water that the legs of the bed were soaking and the matting itself a regular pool. Nor could we make out what kind of washing you’d been having; and for days afterwards we had a laugh over it. But I’ve neither any time to get the water ready; nor do I see the need for you to have a wash along with me. Besides, to-day it’s chilly, and as you’ve had a bath only a little while back, you can very well just now dispense with one. But I’ll draw a basin of water for you to wash your face, and to shampoo your head with. Not long ago, Yuean Yang sent you a few fruits; they were put in that crystal bowl, so This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['Below is a paper. Memorize the paper and answer my question after the paper.\\n The paper begins. \\n Abstract\\nWe propose a new self-supervised method for pre-training the backbone of deep perception models operating on point The clouds: core idea is to train the model on pretext task which is the reconstruction of the surface on which sampled, and to use the underlying latent the 3D points are vectors aS input to the perception head The intuition is that if the network is able to reconstruct the scene surface given only sparse input points, then it probably also captures fragments of semantic information, that can be used to some boost an actual perception task  This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object single-stream pipeline, as detection: In fact, it supports a opposed to most contrastive learning approaches, allowing training on limited resources We conducted extensive exper driving datasets, involving iments on various autonomous very different kinds of lidars, for both semantic segmenta- The results show the effectiveness tion and object detection. of our method to learn useful representations without any annotation, compared to existing approaches:\\nFigure Aggregation of the self-supervised training on lidar cloud (first column) and occupancy prediction datasets. Input point colored by the learned downstream labels\\n1. Introduction\\ncomplement to 2D cameras, lidars directly capture As a vehicle with high accuracy and the 3D environment of low sensitivity to adverse conditions, such as low illumina- They tion, bright sunlight or oncoming headlights. are thus essential sensors for safe autonomous driving:\\npromising direction to address this question is to pre using only unannotated data, e-g-, on train a neural network labelling, and pretext task which does not require manual then to fine-tune the resulting self-supervised pre-trained network for the targeted downstream task(s). With adequate pre-training, the learned network weights are starting good point for further supervised optimization; training a specific downstream task then typically requires fewer annotations to reach the same performance level as if trained from scratch\\nMost state-of-the-art  lidar-based  perception  methods they regard semantic segmentation [21,76, 94] or whether they object detection 44,73,87,90],assume can be trained large annotated datasets. However; annotating 3D data for on such tasks is notoriously costly and time consuming: As data acquisition is much cheaper than data annotation, being able leverage unannotated data to increase the performance O1 to significant reduce the annotation effort is a assel.\\nA number of self-supervised approaches have been very\\nFigure 2. Overview of the approach. The backbone to pre-train produces latent vectors for each input At pre-training time, the latent point: vector are fed into an volumetric occupancy head that classifies query points as full o empty: At semantic training O test time, the same latent vectors are fed into a semantic head, e.g-, for semantic segmentation Or object detection.\\nsuccessful in 2D (images), even reaching the level of su pervised pre-training 12,16,32,36]. Some self-supervised ideas have been proposed for 3D data as well, which are often transpositions in 3D of 2D methods [39,58]. Most of them focus on contrastive learning [64,72, 86, 89,93] which analogous for learns to infer perceptual features that are similar objects while being far apart for dissimilar objects.\\nsign a loss that leads each point to capture enough knowledge to reconstruct its neighborhood (instead of aggregating in formation from neighbors for a more accurate surface recon struction), which instils a taste of semantics in the geometric (3) based on experiments across seven datasets, our task; self-supervised features, that require only limited resources for training (single 16G GPU), outperform state-of-the-art self-supervised features on semantic segmentation; and are on par with these features on object detection_\\nOnly few such methods apply to lidar point clouds, which have the particularity of having very heterogeneous densities \\nIn this work; we propose a totally new pretext task for the self-supervised pre-training of neural networks operating 0n clouds_ We observe that one of the main reasons why point downstream tasks may fail is related to the sparsity of data Indeed, with automotive lidars, 3D points are especially sparse when far from the areas where laser sensor or on beams have a high incidence on the scanned surface In such cases, objects are difficult t0 recognize, and even more so if they (e.g. are small, such as SO-called vulnerable road user pedestrians, bicyclists) and traffic signs.\\n2. Related work\\nSelf-supervision has been the subject of significant research effort; including for image applications. Early methods mostly focused on direct estimation of the trans formation applied to images [22, 30, 60, 63, 92]. More re cently a great boost of performances has been accomplished with contrastive learning [15,59, 78, 85], clustering based 10, 11] and reconstruction-based approaches methods The latter can operate the reconstruction in the feature space [12,16,28,29,32] trying to reconstruct the features issued signal Or in the images domain [4, 36], from a teacher being partially masked input image reconstructed.\\nmostly supervised context, geometric information In shape and visibility information [38] such as object [42,61- have proved to boost detection performance. Our approach visibility-based surface reconstruction as pretext task uses It takes root in the implicit shape rep for self-supervision shapes resentation literature, where are encoded into latent indicating the vectors, that can be decoded into a function Ishape - shape surface volume occupancy or the distance to the The intuitive idea is that if a network is able to properly reconstruct the 3D geometry of a scene from point clouds, then there are chances that it constructs rich features good that can be reused in a number of other contexts, in particular regarding semantic-related tasks.\\n2.1. 2D adaptation to point clouds\\nbeing 2D methods have adapted to cloud, in par point In the dataset presentation ticular for detection. paper of ONCE [54], the authors produce self-supervision base using methods adapted from image self-supervision: lines BYOL [32], SWaV [11] and DeepCluster 10]: Another work, related to MAE [36] (images) or Point-MAE [66] (part segmentation): Voxel-MAE [58] reconstruct the complete voxel grid, given a partially masked input\\nOur contributions are as follows: (1) we combine surface reconstruction and visibility information to create a sensor agnostic and backbone-agnostic pretext task on 3D point self-supervised point features clouds, which produces good for semantic segmentation and object detection; (2) we de\\nOur approach is reconstruction approach: The major supervision signal which is not made difference lies in our by masking an already sparse input; but by estimating the underlying scene surface using sensor information unknown\\n2.3. Occupancy reconstruction\\n2.2. Self-supervision for point clouds:\\nSurface reconstruction is a well studied subject in com putational geometry: Surfaces are usually described either lusing explicit representations (voxels [55, 83], surface point  clouds [1,48,88] or meshes [31,34,47,53,62,80]) or with implicit representations, which define a function over the 3D space from which the surface can be extracted.\\nClassification and part segmentation. Following the same trends as image methods, pretext tasks have been built in order to reconstruct the input point cloud [71,79], estimate global transformation 17,69], contrast between objects 14,23,70, 81] 0r estimate clusters [35,91]. views\\nSemantic segmentation. Scene level pre-training has been using multi-temporal data, for example, by con- tackled trasting point-wise representation which are matched across temporally distinct and registered acquisitions of the two same scene [37,43,86]. Segcontrast 64] extract segments likely to belong to the same object (ground plane using using RANSAC [26], other cluster This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['Below is a paper. Memorize the paper and answer my question after the paper.\\n The paper begins. \\n Abstract\\nWe focus on single-view 3D reconstruction, where the shape, appearance, and camera pose is to reconstruct Igoal single image of from Such an object (Fig: 1) a task has applications in content creation, augmented & virtual reality (ARIVR), robotics, and is also interesting from scientific perspective, as most neural architectures cannot As humans, reason about 3D scenes we learn object pri- ors, abstract representations that allow US to imagine what a partially-observed object would look like from other view points.   Incorporating such knowledge into a model would enable higher forms of 3D reasoning: While early work exploiting annotated on 3D reconstruction has focused on 16,20,57,63,72], e.g. ground-truth 3D shapes or mul data tiple 2D views, more recent work has relaxed the assump required by the task. In particular; there has been effort tions learning this task from single-view collections of images in depicting a specific category (e.g. 17,27,33] a dataset of cars ), and we also follow this line of work\\nNeRF) coupled with GANs rep Neural Radiance Fields resent a promising direction in the area of 3D reconstruc- to their ability to efficiently tion from a single view; owing model arbitrary topologies: Recent work in this area, how- has mostly focused on synthetic datasets where ever; ex- ground-truth poses overlooked and has known, act are estimation, certain   down- is   important  for which pose applications such augmented reality (AR) and Stream as principled end-to-end reconstruc- robotics: We introduce a tion framework for natural images, where accurate ground Our approach recovers truth poses are not available. an SDF-parameterized 3D shape, pose, and appearance from a single image of an object, without exploiting multiple views during training: More specifically; we leverage an uncondi- tional 3D-aware generator; to which we apply a hybrid in- version scheme where a model produces a first guess of the solution which is then refined via optimization. Our frame - image in as few as 10 steps, enabling work can de-render an its use in practical scenarios We demonstrate state-of-the - variety of real and synthetic benchmarks: results on lart\\nMost established 3D representations in the single-view reconstruction literature based deformable   trian - are on 17, 27, 33], although Neural Radiance Fields gle meshes prominent in (NeRF) [1, 39] have recently become more to their ability the broader 3D vision community owing to efficiently model arbitrary topologies. These have been\\nWork done during an internship at Google\\n2. Related work\\ncombined with GANs 18] for unconditional 3D generation produce more perceptually pleas- they tasks [5,6,40,62],as combining the two ing results_ There has also been work on single-view reconstruction task, eg. Pix2NeRF [4], in the settings which is however demonstrated on simple of faces synthetic datasets where perfect ground-truth poses are or available. Furthermore, there has been less focus overall on producing an end-to-end reconstruction system that addi tionally tackles pose estimation (beyond simple settings), applications. is particularly important for which AR In bridge this gap by proposing a our work we more genera NeRF-based end-to-end reconstruction pipeline that tackles both reconstruction and pose estimation, and demonstrate its broader applicability to natural images where poses can- not be accurately estimated. We further characterize the problem by comparing encoder-based approaches (the ma- jority of methods in the single-view reconstruction litera- ture) to inversion-based approaches (which invert gener- optimization), and show that the latter are ator via more suited to real datasets without accurate ground-truth poses_\\nrendering and scene representations   Although Inverse an  established task; 3D reconstruction is the representa- tions and supervision methods used to tackle this problem have evolved throughout the literature. Early approaches have focused o reconstructing shapes using 3D supervi- sion, adopting voxel [16,20,57,63,72], clouds grids point [14], o SDFs [43], and require synthetic datasets  where ground-truth 3D shapes are available. The introduction of differentiable rendering [8, 9,30,35, 36] has enabled a new shape line of work that attempts to reconstruct and texture from single-view datasets, leveraging triangle mesh repre- sentations [2, 8, 17,22,27,33,69]. Each 3D representation however; comes with its own set of trade-offs_ For instance voxels do not scale efficiently with resolution, while trian- gle meshes are efficient but struggle with arbitrary topolo- gies (most works deform a sphere template). In recent de- velopments, implicit representations encode 3D scene as the weights of an MLP that can be queried at specific cO- ordinates, which allows them to model arbitrary topologies using lightweight networks In such a setting; there has been using implicit SDFs [12, 34] work on 3D reconstruction as neural radiance fields (NeRF) [1, 39]. as well Finally, works incorporate additional structural information some e.g. [64] reconstructs  articulated into 3D representations, shapes using skeleton priors, [9,56] disentangle albedo from reflectance, and [61] uses depth cues. These techniques are orthogonal to ours and may positively benefit each other:\\nMotivated by this, we propose a hybrid GAN inversion technique for NeRFs that can be regarded compro as an encoder produces mise between the two: a first guess of the solution (bootstrapping), which is then refined via We further propose a series of technical con- optimization. tributions, including: (i) the adoption of an SDF represen- tation [65] to improve the reconstructed surfaces and facil- regularizers to itate their conversion to triangle meshes, (ii) accelerate inversion; and (iii) the addition of certain equiv- ariances in the model architecture to improve generaliza- can invert an image in as few tion_ We show that we as 10 optimization steps, making OUr approach usable even in Furthermore, we incorporate a constrained scenarios_ prin- cipled pose estimation framework 53] that frames the prob- regression of a canonical representation followed lem as by Perspective-n-Point PnP), and show that it boosts pose without additional data assumptions. estimation accuracy We summarize our main contributions as follows:\\nNeRF-based reconstruction: The standard use-case of NeRF is to encode single scene given multiple 2D views and associated camera poses, which does not necessarily have how lead to learned shared representations_ There ever been attempts at learning an object by training prior category-specific dataset such (e.g. models on collec- For instance, [26, 47] train tion of cars) shared NeRF backbone conditioned on learned latent code for each ob ject instance. [66] tackles reconstruction conditioned on an encoder; although it requires multiple ground-truth image adopt views for supervision an adversarial and does not synthetic setting, thereby relying on accurate poses from datasets and leading to blurry results. [4, 38] adopt an ad versarial setting and only require a single view during train settings ing; they with simple pose distribu- but focus on Finally, there has been work 0 using diffusion mod tions_ synthesis, els [25, 55] and distillation 46] for novel-view though such methods do not explicitly recover a 3D surface\\nWe introduce an end-to-end single-view 3D reconstruc- tion pipeline based on NeRFs. In this setting, we success- fully demonstrate 3609 object reconstruction from natu ral images under the CMR 17] benchmark: hybrid inversion scheme for NeRFs to ac- We propose celerate the reversal of pre-trained 3D-aware generators. Inspired by the literature 0n pose estimation, we propose principled PnP-based pose estimator that leverages our framework and This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgn2004/.local/lib/python3.8/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `[2277, 29937, 13291, 29901]` in the following instance: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "['Below is a paper. Memorize the paper and answer my question after the paper.\\n The paper begins. \\n ABSTRACT\\nWe study the problem of aligning the supports of distributions. Compared to the existing work 0n distribution alignment, support alignment does not require the densities to be matched We propose symmetric support difference as a divergence quantify the mismatch between supports_ We show that select discrimi measure t0 nators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space_ Following this result; our method aligns supports by minimizing a symmetrized optimal transport cost in the discriminator ID space via an adversarial relaxed Furthermore, we show that our approach can be viewed as limit of process_ existing notions of alignment by increasing transportation assignment tolerance We quantitatively evaluate the method across domain adaptation tasks with shifts experiments in label distributions show that the proposed method is more Our robust against these shifts than other alignment-based baselines.\\nINTRODUCTION\\nLearning tasks often involve estimating properties of distributions from samples Or aligning such We can align full distributions (adversarial domain alignment) , certain characteristics across domains_ statistics (canonical correlation analysis), or the support of distributions (this paper). Much of the recent work has focused on full distributional alignment, for In domain adaptation, reasons good motivated by theoretical results (Ben-David et al,, 2007; 2010) , a series of papers (Ajakan et al,, 2014; Ganin & Lempitsky, 2015; Ganin et al,, 2016; Tzeng et al,, 2017; Shen et al,, 2018; Pei et al, 2018; Zhao et al, 2018; Li et al,, 2018a; Wang et al, 2021; Kumar et al,, 2018) seek to align distributions of representations between domains, and utilize a shared classifier on the aligned representation space_\\nAlignment in distributions implies alignment in supports However; when there are additional objec- tives/constraints to satisfy, the minimizer for a distribution alignment objective does not necessarily minimize a support alignment objective. Example in Figure 1 demonstrates the qualitative distinction between two minimizers when distribution alignment is not achievable. The distribution alignment objective prefers to supports unaligned even if support alignment is achievable. Recent works keep (Zhao et al,, 2019; Li et al , 2020; Tan et al,, 2020; Wu et al,, 2019b; Tachet des Combes et al. 2020) have demonstrated that shift in label distributions between source and target leads to characterizable performance drop when the representations are forced into distribution alignment_ The error bound in Johansson et al. (2019) suggests aligning the supports of representations instead.\\nwe focus on distribution support as the key characteristic to align. In this paper; We introduce support divergence to measure the support mismatch and algorithms to optimize such alignment We position OUI approach in the spectrum of other alignment methods. Our contributions are as also follows (all proofs can be found in Appendix A):\\nIn Section 2.1, we measure the differences between supports of distributions. Building on the 1 Hausdorff distance, we introduce a novel support divergence better suited for optimization which we refer to as symmetric support difference (SSD) divergence\\nTong \\'First two authors contributed equally Correspondence to Shangyuan sytong@csail mit. edu): We provide the code reproducing experiment results at https timgaripov / /github com asa\\nidentify Section 2.2, of the discriminator  trained for 2 In important we an property Jensen-Shannon divergence: support differences in the original space of interest are \\'pre- served\" as support differences in the one-dimensional discriminator output space_ algorithm for support alignment, Adversarial Support we present our practical In Section 3 3_ Alignment (ASA) Essentially, based on the analysis presented in Section 2.2, our solution is to align supports in the discriminator ID space, which is computationally efficient distribution alignment, relaxed In Section 4, we place different notions of alignment 4_ spectrum from the point of distribution alignment and support alignment within a coherent view of optimal transport; characterizing their relationships, both theoretically in terms of their objectives and practically in terms of their algorithms. In Section 5, we demonstrate the effectiveness of support alignment in practice for domain 5_ adaptation setting: Compared to other alignment-based baselines, our proposed method is more robust against shifts in label distributions.\\nIllustration of differences between the final configurations of distribution alignment and Figure 1 support alignment procedures p(z) is fixed Beta distribution p(x 4,2) with support Beta(x \\'shifted\" Beta distribution q8 (x) [0, 1]; 2,4) parameterized by 0 with Beta(x is support [0 , 0 + 1]. Panel (a) shows the initial configuration with Oinit Panel (b) shows the result -3. by distribution alignment. Panel (c) shows the result by support alignment. We report Wasserstein (7) and SSD divergence DA (p; distance Dw (p; (1)\\nSSD DIVERGENCE AND SUPPORT ALIGNMENT 2\\nequipped with Borel sigma algebra B and Notation.  We consider an Euclidean space X R Euclidean distance) Let P be the set of probability measures on (X,1) (e.g metric d X xx - R For p € P, the support of p is denoted by supp(p) and is defined as the smallest closed set X C X such that p( X) = 1. fip denotes the pushforward measure of p induced by a measurable mapping f slight abuse of notation; we use p(x) and [fip](t) to denote the densities of measures p and With a respectively, implicitly assuming that the measures are absolutely continuous fip evaluated at x and t point € The distance between subset Y C X is defined as d(v,Y) = infyey dw,y) X and € A). The symmetric difference of two sets A and B is defined as AA B (A | B) U (B =\\n2.1 DIFFERENCE BETWEEN SUPPORTS\\nthey we first need t0 evaluate how different To align the supports of distributions, are.  Similar to distribution divergences like Jensen Shannon divergence, we introduce a notion of support divergence_ divergence? between two distributions in P is a function Ds(. P x P _ R satisfying: A support .) 1) Ds(p, 4) > 0 for all p,q € P; 2) Ds(p,q) = 0 iff supp(p) supp(q).\\ndistribution divergence is sensitive to both density and support differences, While support divergence only needs to detect mismatches in supports, which are subsets of the metric space X .\\n2It is not technically a divergence on the space of distributions, since Ds(p; 0 does not imply p\\nAn example of a distance between subsets of a metric space is the Hausdorff distance: dH (X,Y) d(z:Y)  supyey d(y: X)}. Since it depends only on the greatest distance between _ supcex max point and minimizing this objective for alignment only provides signal to single point: To a set make the optimization less sparse, we consider all points that violate the support alignment criterion and introduce symmetric support difference SSD) divergence:\\nWe note that Our proposed SSD divergence is closely related to Chamfer distanceldivergence (CD) 2021) and Relaxed Word (Fan et al., 2017 Mover\\'s Distance (RWMD) (Kusner Nguyen et al. While both CD and RWMD are stated for discrete points (see Section 6 for further et al,, 2015). comments) , SSD divergence is a general difference measure between arbitrary (discrete O continuous) This distinction, albeit small, is important in our theoretical analysis (Sections 2.2,4.1) distributions_\\n2.2 SUPPORT ALIGNMENT IN ONE-DIMENSIONAL SPACE\\nGoodfellow et al. (2014) showed that the log-loss discriminator f [0, 1], trained to distinguish X Er~p Er~q [log f (x)] + f(x))]) can be used to (sup_ [log(1 samples from distributions p and q estimate the Jensen-Shannon This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: [5] GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss detected at Epoch 0, Step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = (\n",
    "    torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    if not config.is_quantized\n",
    "    else bitsandbytes.optim.AdamW(\n",
    "        model.parameters(), lr=config.lr, is_paged=True, optim_bits=32\n",
    "    )\n",
    ")\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(training_dataloader) * config.num_epochs),\n",
    ")\n",
    "\n",
    "(\n",
    "    model,\n",
    "    optimizer,\n",
    "    training_dataloader,\n",
    "    validation_dataloader,\n",
    "    scheduler,\n",
    ") = accelerator.prepare(\n",
    "    model, optimizer, training_dataloader, validation_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "\n",
    "should_exit = False\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(training_dataloader)):\n",
    "        if epoch == 0 and step < 5:\n",
    "            print(f\"Usage: {monitor.get_gpu_utilization()} GB of GPU\")\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss detected at Epoch {epoch}, Step {step}\")\n",
    "            should_exit = True\n",
    "            break\n",
    "        total_loss += loss.detach().float()\n",
    "        loss = loss / config.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    if should_exit:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    precision_val, recall_val, accuracy_val, f1_val, eval_loss = evaluate(\n",
    "        validation_dataloader\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation Data - Precision: {precision_val}, Recall: {recall_val}, Accuracy: {accuracy_val}, F1: {f1_val}\"\n",
    "    )\n",
    "    eval_epoch_loss = eval_loss / len(validation_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(training_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train\n",
    "# trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# # save model\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompter = Prompter(\"alpaca\")\n",
    "# tokenizer_helper = TokenizerHelper(\n",
    "#     prompter, tokenizer, config.max_length\n",
    "# )\n",
    "# train_data = (\n",
    "#     dataset[\"train\"].shuffle().map(tokenizer_helper.generate_and_tokenize_prompt)\n",
    "# )\n",
    "# val_data = (\n",
    "#     dataset[\"test\"]\n",
    "#     .shuffle()\n",
    "#     .map(tokenizer_helper.generate_and_tokenize_prompt)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# token_lengths = [len(tokens['input_ids']) for tokens in train_data]\n",
    "\n",
    "# # Plotting the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(token_lengths, bins=50)\n",
    "# plt.title(\"Histogram of Tokenized Instruction Lengths\")\n",
    "# plt.xlabel(\"Length of Tokenized Instructions\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.save_to_disk(\"./tokenized_long_qa_train\")\n",
    "# val_data.save_to_disk(\"./tokenized_long_qa_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_loader = torch.utils.data.DataLoader(stream_dataset,\n",
    "#                                                     batch_size=args.batch_size * args.data_group_size,\n",
    "#                                                     shuffle=False,\n",
    "#                                                     num_workers=num_workers,\n",
    "#                                                     pin_memory=True,\n",
    "#                                                     collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity\n",
    "# def compute_perplexity(model, dataloader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     total_length = 0\n",
    "\n",
    "#     for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "#         with torch.no_grad():\n",
    "#             inputs = batch['input_ids'].to(config.device)\n",
    "#             labels = batch['labels'].to(config.device)\n",
    "\n",
    "#             outputs = model(inputs, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             total_loss += loss.item() * inputs.size(0)\n",
    "#             total_length += inputs.size(0)\n",
    "\n",
    "#     avg_loss = total_loss / total_length\n",
    "#     perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "\n",
    "#     return perplexity.item()\n",
    "\n",
    "# # Evaluate the model\n",
    "# perplexity_score = compute_perplexity(model, validation_dataloader)\n",
    "# print(f\"Perplexity: {perplexity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate metrics\n",
    "# def calculate_metrics(preds, labels):\n",
    "#     precision = precision_score(labels, preds, average=\"macro\")\n",
    "#     recall = recall_score(labels, preds, average=\"macro\")\n",
    "#     accuracy = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average=\"macro\")\n",
    "#     return precision, recall, accuracy, f1\n",
    "\n",
    "# # Evaluate a dataloader\n",
    "# def evaluate(dataloader):\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     eval_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for data in tqdm(dataloader):\n",
    "#             batch = {k: v for k, v in data.items()}\n",
    "#             outputs = model(**batch)\n",
    "#             loss = outputs.loss\n",
    "#             eval_loss += loss.detach().float()\n",
    "#             preds = torch.argmax(torch.softmax(outputs.logits, dim=1), dim=1)\n",
    "#             labels = batch[\"labels\"]\n",
    "\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     precision, recall, accuracy, f1 = calculate_metrics(all_preds, all_labels)\n",
    "#     return precision, recall, accuracy, f1, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
