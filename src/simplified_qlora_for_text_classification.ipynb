{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/3.8.6/intel/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from managers import SystemMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "    \"TRANSFORMERS_NO_ADVISORY_WARNINGS\": \"true\",\n",
    "    \"TORCHDYNAMO_DISABLE\": \"1\",\n",
    "    \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "}\n",
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "random.seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baseline usage: 0 GB of GPU'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = SystemMonitor()\n",
    "f\"Baseline usage: {monitor.get_gpu_utilization()} GB of GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "class Configuration:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.keep_fraction = kwargs.get(\"keep_fraction\", 0.9)\n",
    "        self.test_fraction = kwargs.get(\"test_fraction\", 0.2)\n",
    "        self.scratch_path = kwargs.get(\"scratch_path\", \"/scratch/vgn2004\")\n",
    "        self.dataset_path = kwargs.get(\n",
    "            \"dataset_path\",\n",
    "            os.path.join(\n",
    "                self.scratch_path, \"fine_tuning\", \"datasets\", \"disaster_tweets.csv\"\n",
    "            ),\n",
    "        )\n",
    "        self.num_workers = kwargs.get(\"num_workers\", 16)\n",
    "        self.num_virtual_tokens = kwargs.get(\"num_virtual_tokens\", 16)\n",
    "        self.batch_size = kwargs.get(\"batch_size\", 16)\n",
    "        self.lr = kwargs.get(\"lr\", 3e-4)\n",
    "        self.num_epochs = kwargs.get(\"num_epochs\", 5)\n",
    "        self.max_length = kwargs.get(\"max_length\", 128)\n",
    "        self.device = kwargs.get(\"device\", \"cuda\")\n",
    "\n",
    "        self.model_name_or_path = kwargs.get(\"model_name_or_path\", \"facebook/opt-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Configuration()\n",
    "config.keep_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,384 || all params: 1,315,774,464 || trainable%: 0.0012451982044241893\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    PromptTuningInit,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name_or_path, trust_remote_code=True\n",
    ")\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=config.num_virtual_tokens,\n",
    "    prompt_tuning_init_text=\"Classify if the Tweet is about a natural disaster or not:\",\n",
    "    tokenizer_name_or_path=config.model_name_or_path,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=config.dataset_path)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=config.test_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2194311a7a994977885ea68caf4f4246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=16):   0%|          | 0/6090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b272b013b91f4af591a9e2b42a284ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=16):   0%|          | 0/1523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgn2004/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 13, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[\"text\"])\n",
    "    cls = {0: \"normal\", 1: \"disaster\"}\n",
    "    inputs = [f\"Tweet : {x} Label : \" for x in examples[\"text\"]]\n",
    "    targets = [cls[x] for x in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            config.max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (\n",
    "            config.max_length - len(sample_input_ids)\n",
    "        ) + model_inputs[\"attention_mask\"][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (\n",
    "            config.max_length - len(sample_input_ids)\n",
    "        ) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(\n",
    "            model_inputs[\"input_ids\"][i][: config.max_length]\n",
    "        )\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(\n",
    "            model_inputs[\"attention_mask\"][i][: config.max_length]\n",
    "        )\n",
    "        labels[\"input_ids\"][i] = torch.tensor(\n",
    "            labels[\"input_ids\"][i][: config.max_length]\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=config.num_workers,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    processed_datasets[\"train\"],\n",
    "    sampler=torch.utils.data.RandomSampler(processed_datasets[\"train\"]),\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=default_data_collator,\n",
    "    pin_memory=True,\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    processed_datasets[\"test\"],\n",
    "    sampler=torch.utils.data.SequentialSampler(processed_datasets[\"test\"]),\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=default_data_collator,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(training_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [06:49<00:00,  1.07s/it]\n",
      "100%|██████████| 96/96 [00:50<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1820.6173, device='cuda:0') train_epoch_loss=tensor(7.5069, device='cuda:0') eval_ppl=tensor(11.6449, device='cuda:0') eval_epoch_loss=tensor(2.4549, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [06:43<00:00,  1.06s/it]\n",
      "100%|██████████| 96/96 [00:50<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(12.8489, device='cuda:0') train_epoch_loss=tensor(2.5533, device='cuda:0') eval_ppl=tensor(5.1494, device='cuda:0') eval_epoch_loss=tensor(1.6389, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [06:43<00:00,  1.06s/it]\n",
      "100%|██████████| 96/96 [00:50<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(6.6534, device='cuda:0') train_epoch_loss=tensor(1.8951, device='cuda:0') eval_ppl=tensor(4.4703, device='cuda:0') eval_epoch_loss=tensor(1.4975, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [06:43<00:00,  1.06s/it]\n",
      "100%|██████████| 96/96 [00:50<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(5.6010, device='cuda:0') train_epoch_loss=tensor(1.7230, device='cuda:0') eval_ppl=tensor(4.0246, device='cuda:0') eval_epoch_loss=tensor(1.3924, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [06:43<00:00,  1.06s/it]\n",
      "100%|██████████| 96/96 [00:50<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(5.2083, device='cuda:0') train_epoch_loss=tensor(1.6503, device='cuda:0') eval_ppl=tensor(3.9274, device='cuda:0') eval_epoch_loss=tensor(1.3680, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(config.device)\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(training_dataloader)):\n",
    "        batch = {k: v.to(config.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(validation_dataloader)):\n",
    "        batch = {k: v.to(config.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(validation_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(training_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>Tweet : Wreckage everywhere, we are fucked Label : </s>normal<pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    f'Tweet : {\"Wreckage everywhere, we are fucked\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "model.to(config.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = {k: v.to(config.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=10,\n",
    "        eos_token_id=3,\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
