{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import bitsandbytes\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from accelerate import Accelerator, FullyShardedDataParallelPlugin\n",
    "from psutil import Process\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetCount,\n",
    ")\n",
    "\n",
    "class SystemMonitor:\n",
    "    def __init__(self):\n",
    "        # Initialize NVML for GPU monitoring\n",
    "        self.nvml_initialized = SystemMonitor._initialize_nvml()\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_nvml(cls):\n",
    "        try:\n",
    "            nvmlInit()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing NVML: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_ram_usage(self):\n",
    "        return Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    def get_gpu_memory_usage(self):\n",
    "        if not self.nvml_initialized:\n",
    "            print(\"NVML not initialized.\")\n",
    "            return None\n",
    "\n",
    "        gpu_memory_usage = []\n",
    "        try:\n",
    "            gpu_count = nvmlDeviceGetCount()\n",
    "            for i in range(gpu_count):\n",
    "                handle = nvmlDeviceGetHandleByIndex(i)\n",
    "                info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                gpu_memory_usage.append(info.used // 1024 ** 3)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving GPU memory info: {e}\")\n",
    "            return None\n",
    "\n",
    "        return gpu_memory_usage\n",
    "\n",
    "    def get_gpu_utilization(self):\n",
    "        gpu_memory_usages = self.get_gpu_memory_usage()\n",
    "        return gpu_memory_usages if gpu_memory_usages is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4bffa2a4d82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m\"wall_clock_breakdown\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     }\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{input_key}\n",
    "{input}\n",
    "{response_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    input=\"{input}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "ROOT_PATH = \"/scratch/vgn2004\"\n",
    "DEFAULT_TRAINING_DATASET = \"databricks/databricks-dolly-15k\"\n",
    "DEFAULT_SEED = 68\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "\n",
    "def load_training_dataset(\n",
    "        tokenizer,\n",
    "        path_or_dataset: str = DEFAULT_TRAINING_DATASET,\n",
    "        seed: int = DEFAULT_SEED\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    This function is used for preprocessing the databricks-dolly-15k dataset.\n",
    "    To fine-tune on your own dataset, you would need to customize the function.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Loading dataset from {path_or_dataset}\")\n",
    "    dataset = load_dataset(path_or_dataset)[\"train\"]\n",
    "    print(f\"Found {dataset.num_rows} rows\", )\n",
    "\n",
    "    def _reformat_data(rec):\n",
    "        # Each row of databricks-dolly-15k contains fields \"instruction\", \"response\", and optionally the \"context\" field\n",
    "        instruction = rec[\"instruction\"]\n",
    "        response = rec[\"response\"]\n",
    "        context = rec.get(\"context\")\n",
    "\n",
    "        if context:\n",
    "            questions = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, input=context)\n",
    "        else:\n",
    "            questions = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction)\n",
    "\n",
    "        return {\"text\": f\"{questions}\\n{response}\"}\n",
    "\n",
    "    dataset = dataset.map(_reformat_data)\n",
    "\n",
    "    def tokenize_function(allEntries):\n",
    "        return tokenizer(allEntries['text'], truncation=True, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "    dataset = dataset.map(tokenize_function)\n",
    "\n",
    "    split_dataset = dataset.train_test_split(test_size=1000, seed=seed)\n",
    "    train_tokenized_dataset = split_dataset['train']\n",
    "    eval_tokenized_dataset = split_dataset['test']\n",
    "\n",
    "    return train_tokenized_dataset, eval_tokenized_dataset\n",
    "\n",
    "def load_model(\n",
    "        pretrained_model_name_or_path: str = \"NousResearch/Llama-2-7b-hf\",\n",
    "        bf16: bool = False,\n",
    ") -> AutoModelForCausalLM:\n",
    "    print(f\"Loading model for {pretrained_model_name_or_path}\")\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    torch_dtype = torch.bfloat16 if bf16 else torch.float16\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        config=config,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_tokenizer(\n",
    "        pretrained_tokenizer_name_or_path: str = \"NousResearch/Llama-2-7b-hf\",\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_tokenizer_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        revision=REVISION,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def train(\n",
    "        *,\n",
    "        input_model: str,\n",
    "        local_output_dir: str,\n",
    "        dbfs_output_dir: str,\n",
    "        epochs: int,\n",
    "        per_device_train_batch_size: int,\n",
    "        per_device_eval_batch_size: int,\n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        gradient_checkpointing: bool,\n",
    "        gradient_accumulation_steps: int,\n",
    "        local_rank: str,\n",
    "        bf16: bool,\n",
    "        logging_steps: int,\n",
    "        save_steps: int,\n",
    "        max_steps: int,\n",
    "        eval_steps: int,\n",
    "        save_total_limit: int,\n",
    "        warmup_steps: int,\n",
    "):\n",
    "    set_seed(seed)\n",
    "    # Enable tf32 for better performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    tokenizer = get_tokenizer()\n",
    "    train_dataset, val_dataset = load_training_dataset(tokenizer, seed=seed)\n",
    "\n",
    "    model = load_model(pretrained_model_name_or_path=input_model, bf16=bf16)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=local_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=1,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_steps=eval_steps,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=logging_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        max_steps=max_steps,\n",
    "        local_rank=local_rank,\n",
    "        warmup_steps=warmup_steps,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    print(\"Training the model\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Saving Model to {local_output_dir}\")\n",
    "    trainer.save_model(output_dir=local_output_dir)\n",
    "    tokenizer.save_pretrained(local_output_dir)\n",
    "\n",
    "    if dbfs_output_dir:\n",
    "        print(f\"Saving Model to {dbfs_output_dir}\")\n",
    "        trainer.save_model(output_dir=dbfs_output_dir)\n",
    "        tokenizer.save_pretrained(dbfs_output_dir)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kwargs = {\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"AdamW\",\n",
    "      \"params\": {\n",
    "        \"lr\": \"auto\",\n",
    "        \"betas\": \"auto\",\n",
    "        \"eps\": \"auto\",\n",
    "        \"weight_decay\": \"auto\"\n",
    "      }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "      \"type\": \"WarmupLR\",\n",
    "      \"params\": {\n",
    "        \"warmup_min_lr\": \"auto\",\n",
    "        \"warmup_max_lr\": \"auto\",\n",
    "        \"warmup_num_steps\": \"auto\"\n",
    "      }\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False\n",
    "    }\n",
    "    train(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
    "tokenizer.model_max_length = config.seq_length\n",
    "tokenizer.padding_side = \"right\" \n",
    "tokenizer.pad_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
