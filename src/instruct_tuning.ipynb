{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vgn2004/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/scratch/vgn2004/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/scratch/vgn2004/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/scratch/vgn2004/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline usage: [0] GB of GPU\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from accelerate import Accelerator, FullyShardedDataParallelPlugin\n",
    "from psutil import Process\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetCount,\n",
    ")\n",
    "\n",
    "class SystemMonitor:\n",
    "    def __init__(self):\n",
    "        # Initialize NVML for GPU monitoring\n",
    "        self.nvml_initialized = SystemMonitor._initialize_nvml()\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_nvml(cls):\n",
    "        try:\n",
    "            nvmlInit()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing NVML: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_ram_usage(self):\n",
    "        return Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    def get_gpu_memory_usage(self):\n",
    "        if not self.nvml_initialized:\n",
    "            print(\"NVML not initialized.\")\n",
    "            return None\n",
    "\n",
    "        gpu_memory_usage = []\n",
    "        try:\n",
    "            gpu_count = nvmlDeviceGetCount()\n",
    "            for i in range(gpu_count):\n",
    "                handle = nvmlDeviceGetHandleByIndex(i)\n",
    "                info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                gpu_memory_usage.append(info.used // 1024 ** 3)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving GPU memory info: {e}\")\n",
    "            return None\n",
    "\n",
    "        return gpu_memory_usage\n",
    "\n",
    "    def get_gpu_utilization(self):\n",
    "        gpu_memory_usages = self.get_gpu_memory_usage()\n",
    "        return gpu_memory_usages if gpu_memory_usages is not None else None\n",
    "    \n",
    "monitor = SystemMonitor()\n",
    "print(f\"Baseline usage: {monitor.get_gpu_utilization()} GB of GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "set_seed(1001)\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.device_count = torch.cuda.device_count()\n",
    "        self.experiment_name = kwargs.get(\"experiment_name\", \"default_experiment\")\n",
    "        self.keep_fraction = kwargs.get(\"keep_fraction\", 0.99)\n",
    "        self.test_fraction = kwargs.get(\"test_fraction\", 0.2)\n",
    "        self.scratch_path = kwargs.get(\"scratch_path\", \"/scratch/vgn2004\")\n",
    "        self.num_workers = kwargs.get(\"num_workers\", 8)\n",
    "        self.batch_size = kwargs.get(\"batch_size\", 16)\n",
    "        self.lr = kwargs.get(\"lr\", 3e-4)\n",
    "        self.num_epochs = kwargs.get(\"num_epochs\", 5)\n",
    "        self.seq_length = kwargs.get(\"seq_length\", 1024)\n",
    "        self.device = kwargs.get(\"device\", accelerator.device)\n",
    "        self.device_map = kwargs.get(\"device_map\", \"auto\")\n",
    "        self.max_gpu_memory = kwargs.get(\"max_gpu_memory\", \"45080MB\")\n",
    "        # self.device_map = kwargs.get(\"device_map\", {\"\": accelerator.process_index})\n",
    "\n",
    "        self.model_name_or_path = kwargs.get(\n",
    "            \"model_name_or_path\", \"togethercomputer/LLaMA-2-7B-32K\" #\"NousResearch/Llama-2-7b-chat-hf\"\n",
    "        )\n",
    "\n",
    "        self.r = kwargs.get(\"r\", 16)\n",
    "        self.lora_alpha = kwargs.get(\"lora_alpha\", 64)\n",
    "        self.lora_dropout = kwargs.get(\"lora_dropout\", 0.2)\n",
    "        self.lora_bias = kwargs.get(\"lora_bias\", \"none\")\n",
    "        self.is_gradient_checkpointing_enabled = kwargs.get(\n",
    "            \"is_gradient_checkpointing_enabled\", True\n",
    "        )\n",
    "        self.is_gradient_accumulation_enabled = kwargs.get(\n",
    "            \"is_gradient_accumulation_enabled\", True\n",
    "        )\n",
    "        self.gradient_accumulation_steps = kwargs.get(\"gradient_accumulation_steps\", self.batch_size)\n",
    "\n",
    "        self.is_quantized = kwargs.get(\"is_quantized\", True)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(f\"{k}: {v}\" for k, v in vars(self).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWARGS: {'experiment_name': 'default_experiment', 'f': '/home/vgn2004/.local/share/jupyter/runtime/kernel-582b54fd-d51d-496e-ae04-910f89e443cb.json'}\n",
      "Configuration: \n",
      "device_count: 1\n",
      "experiment_name: default_experiment\n",
      "keep_fraction: 0.99\n",
      "test_fraction: 0.2\n",
      "scratch_path: /scratch/vgn2004\n",
      "num_workers: 8\n",
      "batch_size: 16\n",
      "lr: 0.0003\n",
      "num_epochs: 5\n",
      "seq_length: 1024\n",
      "device: cuda\n",
      "device_map: auto\n",
      "max_gpu_memory: 45080MB\n",
      "model_name_or_path: togethercomputer/LLaMA-2-7B-32K\n",
      "r: 16\n",
      "lora_alpha: 64\n",
      "lora_dropout: 0.2\n",
      "lora_bias: none\n",
      "is_gradient_checkpointing_enabled: True\n",
      "is_gradient_accumulation_enabled: True\n",
      "gradient_accumulation_steps: 16\n",
      "is_quantized: True\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Fine-tuning configuration\")\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"default_experiment\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "kwargs = vars(args)\n",
    "kwargs.update(\n",
    "    dict((arg[0].lstrip(\"-\"), arg[1]) for arg in zip(unknown[::2], unknown[1::2]))\n",
    ")\n",
    "print(f\"KWARGS: {kwargs}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# os.environ.update(env_vars)\n",
    "\n",
    "config = Configuration(**kwargs)\n",
    "print(f\"Configuration: \\n{config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "# This is a training prompt that does not contain an input string.  The instruction by itself has enough information\n",
    "# to respond.  For example, the instruction might ask for the year a historic figure was born.\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is a training prompt that contains an input string that serves as context for the instruction.  For example,\n",
    "# the input might be a passage from Wikipedia and the intruction is to extract some information from it.\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{input}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    input=\"{input}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
    "tokenizer.model_max_length = config.seq_length\n",
    "tokenizer.padding_side = \"right\" \n",
    "tokenizer.pad_token, tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 32768\n"
     ]
    }
   ],
   "source": [
    "# Find max allowed sequence length\n",
    "model_config = AutoConfig.from_pretrained(config.model_name_or_path)\n",
    "max_length = None\n",
    "for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "    max_length = getattr(model_config, length_setting, None)\n",
    "    if max_length:\n",
    "        print(f\"Found max lenth: {max_length}\")\n",
    "        break\n",
    "if not max_length:\n",
    "    max_length = 1024\n",
    "    print(f\"Using default max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"togethercomputer/LLaMA-2-7B-32K\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"auto_map\": {\n",
       "    \"AutoModelForCausalLM\": \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"type\": \"linear\"\n",
       "  },\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.max_position_embeddings = config.seq_length\n",
    "model_config.bos_token_id = tokenizer.bos_token_id\n",
    "model_config.eos_token_id = tokenizer.eos_token_id\n",
    "model_config.pad_token_id = tokenizer.pad_token_id\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5fe3b9f69641cd880a5121f426307f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name_or_path,\n",
    "    config=model_config,\n",
    "    device_map=config.device_map,\n",
    "    quantization_config=quantization_config,\n",
    "    max_memory={i: config.max_gpu_memory for i in range(config.device_count)},\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "if config.is_gradient_checkpointing_enabled:\n",
    "    model.config.use_cache = False\n",
    "    model.enable_input_require_grads()\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Model settings\n",
    "model.config.pretraining_tp = 1\n",
    "model.config.torch_dtype = torch.float32\n",
    "setattr(model, 'model_parallel', True)\n",
    "setattr(model, 'is_parallelizable', True)\n",
    "\n",
    "\n",
    "def find_all_linear_names(m):\n",
    "    cls = bitsandbytes.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in m.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=config.r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=config.is_gradient_checkpointing_enabled)\n",
    "# model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"togethercomputer/LLaMA-2-7B-32K\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForCausalLM\": \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32008\n",
      "}\n",
      "\n",
      "torch.float32 262475776 0.07498282840456864\n",
      "torch.uint8 3238002688 0.9250171715954314\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "\n",
    "# Print dtypes\n",
    "dtypes = {}\n",
    "for _, p in model.named_parameters():\n",
    "    dtype = p.dtype\n",
    "    if dtype not in dtypes: dtypes[dtype] = 0\n",
    "    dtypes[dtype] += p.numel()\n",
    "total = 0\n",
    "for k, v in dtypes.items():\n",
    "    total += v\n",
    "for k, v in dtypes.items():\n",
    "    print(k, v, v / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 15 06:14:53 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Quadro RTX 8000                On  | 00000000:2F:00.0 Off |                    0 |\r\n",
      "| N/A   51C    P0              73W / 250W |   5567MiB / 46080MiB |      6%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A   1548930      C   .../apps/python/3.8.6/intel/bin/python     5564MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples):\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "def preprocess_dataset(tokenizer, max_length, dataset):\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "    print(\"Processed dataset has %d rows\", dataset.num_rows)\n",
    "    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "    print(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n",
    "\n",
    "    dataset = dataset.shuffle()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset has %d rows {'train': 15011}\n",
      "Processed dataset has %d rows after filtering for truncated records {'train': 14726}\n",
      "Train data size:  11780\n",
      "Test data size:  2946\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "def _add_text(rec):\n",
    "    instruction = rec[\"instruction\"]\n",
    "    response = rec[\"response\"]\n",
    "    context = rec.get(\"context\")\n",
    "\n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "    if context:\n",
    "        rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "    else:\n",
    "        rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "    return rec\n",
    "\n",
    "dataset = dataset.map(_add_text)\n",
    "processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=config.seq_length, dataset=dataset)\n",
    "split_dataset = processed_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "print(\"Train data size: \", split_dataset[\"train\"].num_rows)\n",
    "print(\"Test data size: \", split_dataset[\"test\"].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/737 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: [5] GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/737 [00:19<3:59:06, 19.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: [12] GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/737 [00:57<6:09:44, 30.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: [22] GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/737 [01:42<7:36:24, 37.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: [29] GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 4/737 [02:00<6:01:44, 29.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: [29] GB of GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/737 [11:34<9:34:35, 48.08s/it]"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")\n",
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    split_dataset[\"train\"],\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    split_dataset[\"test\"],\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = (\n",
    "    torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    if not config.is_quantized\n",
    "    else bitsandbytes.optim.AdamW(\n",
    "        model.parameters(), lr=config.lr, is_paged=True, optim_bits=32\n",
    "    )\n",
    ")\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(training_dataloader) * config.num_epochs),\n",
    ")\n",
    "\n",
    "(\n",
    "    model,\n",
    "    optimizer,\n",
    "    training_dataloader,\n",
    "    validation_dataloader,\n",
    "    scheduler,\n",
    ") = accelerator.prepare(\n",
    "    model, optimizer, training_dataloader, validation_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "\n",
    "should_exit = False\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(training_dataloader)):\n",
    "        if epoch == 0 and step < 5:\n",
    "            print(f\"Usage: {monitor.get_gpu_utilization()} GB of GPU\")\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss detected at Epoch {epoch}, Step {step}\")\n",
    "            should_exit = True\n",
    "            break\n",
    "        total_loss += loss.detach().float()\n",
    "#         loss = loss / config.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "#         if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if should_exit:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    precision_val, recall_val, accuracy_val, f1_val, eval_loss = evaluate(\n",
    "        validation_dataloader\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation Data - Precision: {precision_val}, Recall: {recall_val}, Accuracy: {accuracy_val}, F1: {f1_val}\"\n",
    "    )\n",
    "    eval_epoch_loss = eval_loss / len(validation_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(training_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(\n",
    "        f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\".\",\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#     fp16=True,\n",
    "#     learning_rate=lr,\n",
    "#     num_train_epochs=epochs,\n",
    "#     deepspeed=deepspeed,\n",
    "#     gradient_checkpointing=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=eval_steps,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=save_steps,\n",
    "#     save_total_limit=save_total_limit,\n",
    "#     load_best_model_at_end=False,\n",
    "#     disable_tqdm=True,\n",
    "#     remove_unused_columns=False,\n",
    "#     local_rank=local_rank,\n",
    "#     warmup_steps=warmup_steps,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "#     train_dataset=split_dataset[\"train\"],\n",
    "#     eval_dataset=split_dataset[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.save_model(output_dir=local_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
