project: qlora_classification
program: qlora_runner_for_text_classification.py
method: bayes
metric:
  goal: minimize
  name: eval_loss
parameters:
  config_path:
    value: "wandb"
  user_config.env:
    value: "big_qlora_classification_sweep_with_gradient_checkpointing"
  train_config.epochs:
    value: 5
  tokenizer_config.tokenizer_name:
    value: "classification_with_gradient_checkpointing"
  tokenizer_config.max_tokens:
    value: 1024
  model_name:
    value: "facebook/opt-6.7b"
  dataset_name:
    value: "paper_abstract_topic_prediction"
  lora_config.task_type:
    value: "SEQ_CLS"
  keep_fraction:
    value: 0.9
  train_config.optim_bits:
    value: 8
  quantization_config.bnb_4bit_use_double_quant:
    value: true
  batch_size:
    values: [ 4, 8, 16, 32]
  lora_config.r:
    values: [ 32, 64, 128 ]
  lora_config.lora_alpha:
    values: [ 64, 128 ]
  lora_config.lora_dropout:
    values: [ 0.2, 0.25, 0.3, 0.4, 0.5, 0.6]
  train_config.lr:
    values: [ 5e-3, 8e-3, 1e-4, 2e-4, 5e-4]